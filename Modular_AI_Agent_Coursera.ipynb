{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNC9hJxwgkYS5PkM82bqC5I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/F-Bafti/AI-Agents-and-Agentic-AI/blob/master/Modular_AI_Agent_Coursera.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modular AI Agent\n",
        "## G- Goal Implementation"
      ],
      "metadata": {
        "id": "PpbBL3Syp6Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community\n",
        "!pip uninstall cohere langchain-cohere -y\n",
        "\n",
        "!pip install cohere>=5.0.0\n",
        "!pip install langchain-cohere\n",
        "\n",
        "!pip install cohere==5.11.0 langchain-cohere==0.3.0\n",
        "\n",
        "import json, os\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('COHERE_API_KEY')\n",
        "os.environ['COHERE_API_KEY'] = api_key"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hvoMegosp88Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Prompt Class and Agent Response"
      ],
      "metadata": {
        "id": "FKWqNkoWApOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Any, Callable, Optional\n",
        "import json\n",
        "import time\n",
        "import traceback\n",
        "from langchain_cohere import ChatCohere\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Prompt:\n",
        "    messages: List[Dict] = field(default_factory=list)\n",
        "    tools: List[Dict] = field(default_factory=list)\n",
        "    metadata: dict = field(default_factory=dict)  # Fixing mutable default issue\n",
        "\n",
        "\n",
        "def generate_response(prompt: Prompt) -> str:\n",
        "    \"\"\"Call LLM to get response\"\"\"\n",
        "\n",
        "    # Initialize Cohere model with LangChain\n",
        "    llm = ChatCohere(\n",
        "        model=\"command-r-plus\",  # or \"command-r\" for faster responses\n",
        "        max_tokens=1024,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    messages = prompt.messages\n",
        "    tools = prompt.tools\n",
        "\n",
        "    result = None\n",
        "\n",
        "    # Convert dict messages to LangChain message objects\n",
        "    # For OpenAI to LangChain format\n",
        "    langchain_messages = []\n",
        "    for msg in messages:\n",
        "        if msg[\"role\"] == \"system\":\n",
        "            langchain_messages.append(SystemMessage(content=msg[\"content\"]))\n",
        "        elif msg[\"role\"] == \"user\":\n",
        "            langchain_messages.append(HumanMessage(content=msg[\"content\"]))\n",
        "        elif msg[\"role\"] == \"assistant\":\n",
        "            langchain_messages.append(AIMessage(content=msg[\"content\"]))\n",
        "\n",
        "    # If there is no tools for LLM to use just get the text response\n",
        "    if not tools:\n",
        "        response = llm.invoke(langchain_messages)\n",
        "        result = response.content\n",
        "    else:\n",
        "        # Part1: Handeling tools\n",
        "        # Convert tools to the format expected by LangChain Cohere\n",
        "        # Conversion from OpenAI to LangChain expected format for tool\n",
        "        formatted_tools = []\n",
        "        for tool in tools:\n",
        "            if isinstance(tool, dict) and \"function\" in tool:\n",
        "                # Extract the function definition for LangChain\n",
        "                func_def = tool[\"function\"]\n",
        "                # Convert to Cohere-compatible format\n",
        "                cohere_tool = {\n",
        "                    \"title\": func_def[\"name\"],\n",
        "                    \"description\": func_def[\"description\"],\n",
        "                    \"properties\": func_def[\"parameters\"].get(\"properties\", {}),\n",
        "                    \"required\": func_def[\"parameters\"].get(\"required\", [])\n",
        "                }\n",
        "                formatted_tools.append(cohere_tool)\n",
        "\n",
        "        # Part2: Handeling tools\n",
        "        # Bind tools to the model for function calling\n",
        "        llm_with_tools = llm.bind_tools(formatted_tools)\n",
        "        response = llm_with_tools.invoke(langchain_messages)\n",
        "\n",
        "        # Part3: Handeling tools\n",
        "        # If there is a tool, take the response of LLM and extract the tool\n",
        "        # Otherwise just take the text from the LLM response\n",
        "        if response.tool_calls:\n",
        "            tool_call = response.tool_calls[0]\n",
        "            result = {\n",
        "                \"tool\": tool_call[\"name\"],\n",
        "                \"args\": tool_call[\"args\"],\n",
        "            }\n",
        "            result = json.dumps(result)\n",
        "        else:\n",
        "            result = response.content\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyCk9AJy2QXE",
        "outputId": "1c19b57a-85e1-4ac2-ae71-e3719179e729"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
            "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up the GAME(Goal, Action, Memory, Environment)"
      ],
      "metadata": {
        "id": "rJwRLroIB3Us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1- Goal Class\n",
        "# Creates a simple container to hold information about what the agent should accomplish\n",
        "# priority: How important this goal is (lower numbers = higher priority)\n",
        "# name: Short name for the goal (like \"Gather Information\")\n",
        "# description: Detailed explanation of what to do\n",
        "# frozen=True: Makes this immutable - once created, it can't be changed\n",
        "@dataclass(frozen=True)\n",
        "class Goal:\n",
        "    priority: int\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "\n",
        "\n",
        "# 2- Action Class\n",
        "# Creates a wrapper around a Python function to make it available to the AI\n",
        "# name: What the AI will call this action (like \"read_file\")\n",
        "# function: The actual Python function to execute\n",
        "# description: Tells the AI what this function does\n",
        "# parameters: Describes what arguments the function needs (JSON schema format)\n",
        "# terminal: Whether calling this action should end the agent's execution\n",
        "class Action:\n",
        "    def __init__(self,\n",
        "                 name: str,\n",
        "                 function: Callable,\n",
        "                 description: str,\n",
        "                 parameters: Dict,\n",
        "                 terminal: bool = False):\n",
        "        self.name = name\n",
        "        self.function = function\n",
        "        self.description = description\n",
        "        self.terminal = terminal\n",
        "        self.parameters = parameters\n",
        "\n",
        "    def execute(self, **args) -> Any:\n",
        "        \"\"\"Execute the action's function\"\"\"\n",
        "        return self.function(**args)\n",
        "\n",
        "# Creates a container to store all available actions\n",
        "class ActionRegistry:\n",
        "    def __init__(self):\n",
        "        self.actions = {}\n",
        "\n",
        "    def register(self, action: Action):\n",
        "        self.actions[action.name] = action\n",
        "\n",
        "    # Looks up an action by its name\n",
        "    def get_action(self, name: str) -> Action | None:\n",
        "        return self.actions.get(name, None)\n",
        "\n",
        "    # Returns ALL actions as a list\n",
        "    def get_actions(self) -> List[Action]:\n",
        "        \"\"\"Get all registered actions\"\"\"\n",
        "        return list(self.actions.values())\n",
        "\n",
        "\n",
        "\n",
        "# 3- Memory Class\n",
        "# Creates a container to store the conversation history\n",
        "# self.items = []: An empty list to hold memory items\n",
        "# Each item will be a dictionary representing one piece of the conversation\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.items = []  # Basic conversation history\n",
        "\n",
        "    # Adds a new memory item to the end of the list\n",
        "    def add_memory(self, memory: dict):\n",
        "        \"\"\"Add memory to working memory\"\"\"\n",
        "        self.items.append(memory)\n",
        "\n",
        "    # Returns the stored memories as a list\n",
        "    def get_memories(self, limit: int = None) -> List[Dict]:\n",
        "        \"\"\"Get formatted conversation history for prompt\"\"\"\n",
        "        return self.items[:limit]\n",
        "\n",
        "    # Creates a new Memory object with system messages filtered out\n",
        "    def copy_without_system_memories(self):\n",
        "        \"\"\"Return a copy of the memory without system memories\"\"\"\n",
        "        filtered_items = [m for m in self.items if m[\"type\"] != \"system\"]\n",
        "        memory = Memory()\n",
        "        memory.items = filtered_items\n",
        "        return memory\n",
        "\n",
        "\n",
        "\n",
        "# 4- Environment Class\n",
        "# This is where actions actually get executed safely\n",
        "# try: attempts to run the action\n",
        "# action.execute(**args) calls the action with the provided arguments\n",
        "# If it works: calls self.format_result() to package the result nicely\n",
        "# If it fails: catches the error and returns error information instead of crashing\n",
        "class Environment:\n",
        "    def execute_action(self, action: Action, args: dict) -> dict:\n",
        "        \"\"\"Execute an action and return the result.\"\"\"\n",
        "        try:\n",
        "            result = action.execute(**args)\n",
        "            return self.format_result(result)\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"tool_executed\": False,\n",
        "                \"error\": str(e),\n",
        "                \"traceback\": traceback.format_exc()\n",
        "            }\n",
        "\n",
        "    def format_result(self, result: Any) -> dict:\n",
        "        \"\"\"Format the result with metadata.\"\"\"\n",
        "        return {\n",
        "            \"tool_executed\": True,\n",
        "            \"result\": result,\n",
        "            \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
        "        }\n"
      ],
      "metadata": {
        "id": "oKY-KLx8B2pW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the Agent Language and Function-Calling"
      ],
      "metadata": {
        "id": "7C5YsptXHFyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What this class do?\n",
        "# Example: user_input = \"Write a README for this project.\"\n",
        "# What construct_prompt Does:\n",
        "# The agent takes that simple user input and builds a complex prompt that includes:\n",
        "# System instructions (the goals), Conversation history, Available tools, The user's request\n",
        "# Prompt(\n",
        "#     messages=[\n",
        "#         {\"role\": \"system\", \"content\": \"Goal: Read each file...\"},\n",
        "#         {\"role\": \"user\", \"content\": \"Write a README for this project.\"},\n",
        "#         {\"role\": \"assistant\", \"content\": '{\"tool\": \"list_files\", \"args\": {}}'},\n",
        "#         {\"role\": \"user\", \"content\": \"Tool result: [file1.py, file2.py]\"}\n",
        "#     ],\n",
        "#     tools=[list_files_tool, read_file_tool, terminate_tool]\n",
        "# )\n",
        "\n",
        "class AgentLanguage:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def construct_prompt(self,\n",
        "                         actions: List[Action],\n",
        "                         environment: Environment,\n",
        "                         goals: List[Goal],\n",
        "                         memory: Memory) -> Prompt:\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "    def parse_response(self, response: str) -> dict:\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "\n",
        "\n",
        "# This is a concrete implementation of the abstract AgentLanguage class\n",
        "class AgentFunctionCallingActionLanguage(AgentLanguage):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # Takes a list of Goal objects and converts them into a system message\n",
        "    # Creates a formatted string with separators to make it readable\n",
        "    # Returns a list with one system message containing all goals\n",
        "    def format_goals(self, goals: List[Goal]) -> List:\n",
        "        # Map all goals to a single string that concatenates their description\n",
        "        # and combine into a single message of type system\n",
        "        sep = \"\\n-------------------\\n\"\n",
        "        goal_instructions = \"\\n\\n\".join([f\"{goal.name}:{sep}{goal.description}{sep}\" for goal in goals])\n",
        "        return [\n",
        "            {\"role\": \"system\", \"content\": goal_instructions}\n",
        "        ]\n",
        "\n",
        "\n",
        "    def format_memory(self, memory: Memory) -> List:\n",
        "        \"\"\"Generate response from language model\"\"\"\n",
        "        # Map all environment results to a role:user messages\n",
        "        # Map all assistant messages to a role:assistant messages\n",
        "        # Map all user messages to a role:user messages\n",
        "        items = memory.get_memories()\n",
        "        mapped_items = []\n",
        "        for item in items:\n",
        "\n",
        "            content = item.get(\"content\", None)\n",
        "            if not content:\n",
        "                content = json.dumps(item, indent=4)\n",
        "\n",
        "            if item[\"type\"] == \"assistant\":\n",
        "                mapped_items.append({\"role\": \"assistant\", \"content\": content})\n",
        "            elif item[\"type\"] == \"environment\":\n",
        "                # Map environment results to user messages for Cohere compatibility\n",
        "                mapped_items.append({\"role\": \"user\", \"content\": f\"Tool result: {content}\"})\n",
        "            else:\n",
        "                mapped_items.append({\"role\": \"user\", \"content\": content})\n",
        "\n",
        "        return mapped_items\n",
        "\n",
        "    def format_actions(self, actions: List[Action]) -> List:\n",
        "        \"\"\"Convert actions to LangChain-compatible tool format\"\"\"\n",
        "\n",
        "        tools = []\n",
        "        for action in actions:\n",
        "            # Convert to OpenAI function format that LangChain can use\n",
        "            tool_def = {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": action.name,\n",
        "                    \"description\": action.description[:1024],\n",
        "                    \"parameters\": action.parameters,\n",
        "                }\n",
        "            }\n",
        "            tools.append(tool_def)\n",
        "\n",
        "        return tools\n",
        "\n",
        "    def construct_prompt(self,\n",
        "                         actions: List[Action],\n",
        "                         environment: Environment,\n",
        "                         goals: List[Goal],\n",
        "                         memory: Memory) -> Prompt:\n",
        "\n",
        "        prompt = []\n",
        "        prompt += self.format_goals(goals)\n",
        "        prompt += self.format_memory(memory)\n",
        "\n",
        "        tools = self.format_actions(actions)\n",
        "\n",
        "        return Prompt(messages=prompt, tools=tools)\n",
        "\n",
        "    def adapt_prompt_after_parsing_error(self,\n",
        "                                         prompt: Prompt,\n",
        "                                         response: str,\n",
        "                                         traceback: str,\n",
        "                                         error: Any,\n",
        "                                         retries_left: int) -> Prompt:\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def parse_response(self, response: str) -> dict:\n",
        "        \"\"\"Parse LLM response into structured format by extracting the ```json block\"\"\"\n",
        "\n",
        "        try:\n",
        "            return json.loads(response)\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"tool\": \"terminate\",\n",
        "                \"args\": {\"message\": response}\n",
        "            }\n",
        "\n"
      ],
      "metadata": {
        "id": "VIfRARNp2RfO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Agent Class"
      ],
      "metadata": {
        "id": "y4eONEZiOFsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self,\n",
        "                 goals: List[Goal],\n",
        "                 agent_language: AgentLanguage,\n",
        "                 action_registry: ActionRegistry,\n",
        "                 generate_response: Callable[[Prompt], str],\n",
        "                 environment: Environment):\n",
        "        \"\"\"\n",
        "        Initialize an agent with its core GAME components\n",
        "        \"\"\"\n",
        "        self.goals = goals\n",
        "        self.generate_response = generate_response\n",
        "        self.agent_language = agent_language\n",
        "        self.actions = action_registry\n",
        "        self.environment = environment\n",
        "\n",
        "    def construct_prompt(self, goals: List[Goal], memory: Memory, actions: ActionRegistry) -> Prompt:\n",
        "        \"\"\"Build prompt with memory context\"\"\"\n",
        "        return self.agent_language.construct_prompt(\n",
        "            actions=actions.get_actions(),\n",
        "            environment=self.environment,\n",
        "            goals=goals,\n",
        "            memory=memory\n",
        "        )\n",
        "\n",
        "    def get_action(self, response):\n",
        "        invocation = self.agent_language.parse_response(response)\n",
        "        action = self.actions.get_action(invocation[\"tool\"])\n",
        "        return action, invocation\n",
        "\n",
        "    def should_terminate(self, response: str) -> bool:\n",
        "        action_def, _ = self.get_action(response)\n",
        "        return action_def.terminal\n",
        "\n",
        "    def set_current_task(self, memory: Memory, task: str):\n",
        "        memory.add_memory({\"type\": \"user\", \"content\": task})\n",
        "\n",
        "    def update_memory(self, memory: Memory, response: str, result: dict):\n",
        "        \"\"\"\n",
        "        Update memory with the agent's decision and the environment's response.\n",
        "        \"\"\"\n",
        "        new_memories = [\n",
        "            {\"type\": \"assistant\", \"content\": response},\n",
        "            {\"type\": \"environment\", \"content\": json.dumps(result)}\n",
        "        ]\n",
        "        for m in new_memories:\n",
        "            memory.add_memory(m)\n",
        "\n",
        "    def prompt_llm_for_action(self, full_prompt: Prompt) -> str:\n",
        "        response = self.generate_response(full_prompt)\n",
        "        return response\n",
        "\n",
        "    def run(self, user_input: str, memory=None, max_iterations: int = 50) -> Memory:\n",
        "        \"\"\"\n",
        "        Execute the GAME loop for this agent with a maximum iteration limit.\n",
        "        \"\"\"\n",
        "        memory = memory or Memory()\n",
        "        self.set_current_task(memory, user_input)\n",
        "\n",
        "        for _ in range(max_iterations):\n",
        "            # Construct a prompt that includes the Goals, Actions, and the current Memory\n",
        "            prompt = self.construct_prompt(self.goals, memory, self.actions)\n",
        "\n",
        "            print(\"Agent thinking...\")\n",
        "            # Generate a response from the agent\n",
        "            response = self.prompt_llm_for_action(prompt)\n",
        "            print(f\"Agent Decision: {response}\")\n",
        "\n",
        "            # Determine which action the agent wants to execute\n",
        "            action, invocation = self.get_action(response)\n",
        "\n",
        "            # Execute the action in the environment\n",
        "            result = self.environment.execute_action(action, invocation[\"args\"])\n",
        "            print(f\"Action Result: {result}\")\n",
        "\n",
        "            # Update the agent's memory with information about what happened\n",
        "            self.update_memory(memory, response, result)\n",
        "\n",
        "            # Check if the agent has decided to terminate\n",
        "            if self.should_terminate(response):\n",
        "                break\n",
        "\n",
        "        return memory"
      ],
      "metadata": {
        "id": "Y7QZrPaoNRpk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Code"
      ],
      "metadata": {
        "id": "wc5S2XMTRAm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the agent's goals\n",
        "goals = [\n",
        "    Goal(priority=1, name=\"Gather Information\", description=\"Read each file in the project\"),\n",
        "    Goal(priority=1, name=\"Terminate\", description=\"Call the terminate call when you have read all the files \"\n",
        "         \"and provide the content of the README in the terminate message\")\n",
        "]\n",
        "\n",
        "# Define the agent's language\n",
        "agent_language = AgentFunctionCallingActionLanguage()\n",
        "\n",
        "def read_project_file(name: str) -> str:\n",
        "    with open(name, \"r\") as f:\n",
        "        return f.read()\n",
        "\n",
        "def list_project_files() -> List[str]:\n",
        "    try:\n",
        "        # Get all files (not just .py files) to see what's available\n",
        "        all_files = [f for f in os.listdir(\".\") if os.path.isfile(f)]\n",
        "        py_files = [f for f in all_files if f.endswith(\".py\")]\n",
        "\n",
        "        if not py_files:\n",
        "            # If no .py files, return all text files\n",
        "            text_files = [f for f in all_files if f.endswith(('.txt', '.md', '.json', '.yaml', '.yml', '.py', '.js', '.html', '.css'))]\n",
        "            return sorted(text_files) if text_files else [\"No readable files found in current directory\"]\n",
        "\n",
        "        return sorted(py_files)\n",
        "    except Exception as e:\n",
        "        return [f\"Error listing files: {str(e)}\"]\n",
        "\n",
        "# Define the action registry and register some actions\n",
        "action_registry = ActionRegistry()\n",
        "\n",
        "action_registry.register(Action(\n",
        "    name=\"list_project_files\",\n",
        "    function=list_project_files,\n",
        "    description=\"Lists all readable files in the project directory (prioritizing Python files, but includes other text files if no Python files exist).\",\n",
        "    parameters={\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {},\n",
        "        \"required\": []\n",
        "    },\n",
        "    terminal=False\n",
        "))\n",
        "\n",
        "action_registry.register(Action(\n",
        "    name=\"read_project_file\",\n",
        "    function=read_project_file,\n",
        "    description=\"Reads a file from the project.\",\n",
        "    parameters={\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"name\": {\"type\": \"string\"}\n",
        "        },\n",
        "        \"required\": [\"name\"]\n",
        "    },\n",
        "    terminal=False\n",
        "))\n",
        "\n",
        "action_registry.register(Action(\n",
        "    name=\"terminate\",\n",
        "    function=lambda message: f\"{message}\\nTerminating...\",\n",
        "    description=\"Terminates the session and prints the message to the user.\",\n",
        "    parameters={\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"message\": {\"type\": \"string\"}\n",
        "        },\n",
        "        \"required\": [\"message\"]\n",
        "    },\n",
        "    terminal=True\n",
        "))\n",
        "\n",
        "# Define the environment\n",
        "environment = Environment()\n",
        "\n",
        "# Create an agent instance\n",
        "agent = Agent(goals, agent_language, action_registry, generate_response, environment)\n",
        "\n",
        "# Run the agent with user input\n",
        "if __name__ == \"__main__\":\n",
        "    user_input = \"Write a README for this project.\"\n",
        "    final_memory = agent.run(user_input)\n",
        "\n",
        "    # Print the final memory\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL MEMORY:\")\n",
        "    print(\"=\"*60)\n",
        "    for memory in final_memory.get_memories():\n",
        "        print(memory)\n",
        "        print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l38Eto2YNWAT",
        "outputId": "f7970679-6296-4219-d4e2-4a2bfb8e96ba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent thinking...\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['main.py'], 'timestamp': '2025-07-24T21:10:55+0000'}\n",
            "Agent thinking...\n",
            "Agent Decision: {\"tool\": \"read_project_file\", \"args\": {\"name\": \"main.py\"}}\n",
            "Action Result: {'tool_executed': True, 'result': 'import os\\nfrom dataclasses import dataclass, field\\nfrom typing import List, Dict, Any, Callable, Optional\\nimport json\\nimport time\\nimport traceback\\nfrom langchain_cohere import ChatCohere\\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\\n\\n@dataclass\\nclass Prompt:\\n    messages: List[Dict] = field(default_factory=list)\\n    tools: List[Dict] = field(default_factory=list)\\n    metadata: dict = field(default_factory=dict)  # Fixing mutable default issue\\n\\n\\ndef generate_response(prompt: Prompt) -> str:\\n    \"\"\"Call LLM to get response\"\"\"\\n    \\n    # Initialize Cohere model with LangChain\\n    llm = ChatCohere(\\n        model=\"command-r-plus\",  # or \"command-r\" for faster responses\\n        max_tokens=1024,\\n        temperature=0.3\\n    )\\n\\n    messages = prompt.messages\\n    tools = prompt.tools\\n\\n    result = None\\n\\n    # Convert dict messages to LangChain message objects\\n    langchain_messages = []\\n    for msg in messages:\\n        if msg[\"role\"] == \"system\":\\n            langchain_messages.append(SystemMessage(content=msg[\"content\"]))\\n        elif msg[\"role\"] == \"user\":\\n            langchain_messages.append(HumanMessage(content=msg[\"content\"]))\\n        elif msg[\"role\"] == \"assistant\":\\n            langchain_messages.append(AIMessage(content=msg[\"content\"]))\\n\\n    if not tools:\\n        response = llm.invoke(langchain_messages)\\n        result = response.content\\n    else:\\n        # Convert tools to the format expected by LangChain Cohere\\n        formatted_tools = []\\n        for tool in tools:\\n            if isinstance(tool, dict) and \"function\" in tool:\\n                # Extract the function definition for LangChain\\n                func_def = tool[\"function\"]\\n                # Convert to Cohere-compatible format\\n                cohere_tool = {\\n                    \"title\": func_def[\"name\"],\\n                    \"description\": func_def[\"description\"],\\n                    \"properties\": func_def[\"parameters\"].get(\"properties\", {}),\\n                    \"required\": func_def[\"parameters\"].get(\"required\", [])\\n                }\\n                formatted_tools.append(cohere_tool)\\n        \\n        # Bind tools to the model for function calling\\n        llm_with_tools = llm.bind_tools(formatted_tools)\\n        response = llm_with_tools.invoke(langchain_messages)\\n\\n        if response.tool_calls:\\n            tool_call = response.tool_calls[0]\\n            result = {\\n                \"tool\": tool_call[\"name\"],\\n                \"args\": tool_call[\"args\"],\\n            }\\n            result = json.dumps(result)\\n        else:\\n            result = response.content\\n\\n    return result\\n\\n\\n@dataclass(frozen=True)\\nclass Goal:\\n    priority: int\\n    name: str\\n    description: str\\n\\n\\nclass Action:\\n    def __init__(self,\\n                 name: str,\\n                 function: Callable,\\n                 description: str,\\n                 parameters: Dict,\\n                 terminal: bool = False):\\n        self.name = name\\n        self.function = function\\n        self.description = description\\n        self.terminal = terminal\\n        self.parameters = parameters\\n\\n    def execute(self, **args) -> Any:\\n        \"\"\"Execute the action\\'s function\"\"\"\\n        return self.function(**args)\\n\\n\\nclass ActionRegistry:\\n    def __init__(self):\\n        self.actions = {}\\n\\n    def register(self, action: Action):\\n        self.actions[action.name] = action\\n\\n    def get_action(self, name: str) -> Action | None:\\n        return self.actions.get(name, None)\\n\\n    def get_actions(self) -> List[Action]:\\n        \"\"\"Get all registered actions\"\"\"\\n        return list(self.actions.values())\\n\\n\\nclass Memory:\\n    def __init__(self):\\n        self.items = []  # Basic conversation history\\n\\n    def add_memory(self, memory: dict):\\n        \"\"\"Add memory to working memory\"\"\"\\n        self.items.append(memory)\\n\\n    def get_memories(self, limit: int = None) -> List[Dict]:\\n        \"\"\"Get formatted conversation history for prompt\"\"\"\\n        return self.items[:limit]\\n\\n    def copy_without_system_memories(self):\\n        \"\"\"Return a copy of the memory without system memories\"\"\"\\n        filtered_items = [m for m in self.items if m[\"type\"] != \"system\"]\\n        memory = Memory()\\n        memory.items = filtered_items\\n        return memory\\n\\n\\nclass Environment:\\n    def execute_action(self, action: Action, args: dict) -> dict:\\n        \"\"\"Execute an action and return the result.\"\"\"\\n        try:\\n            result = action.execute(**args)\\n            return self.format_result(result)\\n        except Exception as e:\\n            return {\\n                \"tool_executed\": False,\\n                \"error\": str(e),\\n                \"traceback\": traceback.format_exc()\\n            }\\n\\n    def format_result(self, result: Any) -> dict:\\n        \"\"\"Format the result with metadata.\"\"\"\\n        return {\\n            \"tool_executed\": True,\\n            \"result\": result,\\n            \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\\n        }\\n\\n\\nclass AgentLanguage:\\n    def __init__(self):\\n        pass\\n\\n    def construct_prompt(self,\\n                         actions: List[Action],\\n                         environment: Environment,\\n                         goals: List[Goal],\\n                         memory: Memory) -> Prompt:\\n        raise NotImplementedError(\"Subclasses must implement this method\")\\n\\n    def parse_response(self, response: str) -> dict:\\n        raise NotImplementedError(\"Subclasses must implement this method\")\\n\\n\\nclass AgentFunctionCallingActionLanguage(AgentLanguage):\\n\\n    def __init__(self):\\n        super().__init__()\\n\\n    def format_goals(self, goals: List[Goal]) -> List:\\n        # Map all goals to a single string that concatenates their description\\n        # and combine into a single message of type system\\n        sep = \"\\\\n-------------------\\\\n\"\\n        goal_instructions = \"\\\\n\\\\n\".join([f\"{goal.name}:{sep}{goal.description}{sep}\" for goal in goals])\\n        return [\\n            {\"role\": \"system\", \"content\": goal_instructions}\\n        ]\\n\\n    def format_memory(self, memory: Memory) -> List:\\n        \"\"\"Generate response from language model\"\"\"\\n        # Map all environment results to a role:user messages\\n        # Map all assistant messages to a role:assistant messages\\n        # Map all user messages to a role:user messages\\n        items = memory.get_memories()\\n        mapped_items = []\\n        for item in items:\\n\\n            content = item.get(\"content\", None)\\n            if not content:\\n                content = json.dumps(item, indent=4)\\n\\n            if item[\"type\"] == \"assistant\":\\n                mapped_items.append({\"role\": \"assistant\", \"content\": content})\\n            elif item[\"type\"] == \"environment\":\\n                # Map environment results to user messages for Cohere compatibility\\n                mapped_items.append({\"role\": \"user\", \"content\": f\"Tool result: {content}\"})\\n            else:\\n                mapped_items.append({\"role\": \"user\", \"content\": content})\\n\\n        return mapped_items\\n\\n    def format_actions(self, actions: List[Action]) -> List:\\n        \"\"\"Convert actions to LangChain-compatible tool format\"\"\"\\n        \\n        tools = []\\n        for action in actions:\\n            # Convert to OpenAI function format that LangChain can use\\n            tool_def = {\\n                \"type\": \"function\",\\n                \"function\": {\\n                    \"name\": action.name,\\n                    \"description\": action.description[:1024],\\n                    \"parameters\": action.parameters,\\n                }\\n            }\\n            tools.append(tool_def)\\n\\n        return tools\\n\\n    def construct_prompt(self,\\n                         actions: List[Action],\\n                         environment: Environment,\\n                         goals: List[Goal],\\n                         memory: Memory) -> Prompt:\\n\\n        prompt = []\\n        prompt += self.format_goals(goals)\\n        prompt += self.format_memory(memory)\\n\\n        tools = self.format_actions(actions)\\n\\n        return Prompt(messages=prompt, tools=tools)\\n\\n    def adapt_prompt_after_parsing_error(self,\\n                                         prompt: Prompt,\\n                                         response: str,\\n                                         traceback: str,\\n                                         error: Any,\\n                                         retries_left: int) -> Prompt:\\n\\n        return prompt\\n\\n    def parse_response(self, response: str) -> dict:\\n        \"\"\"Parse LLM response into structured format by extracting the ```json block\"\"\"\\n\\n        try:\\n            return json.loads(response)\\n\\n        except Exception as e:\\n            return {\\n                \"tool\": \"terminate\",\\n                \"args\": {\"message\": response}\\n            }\\n\\n\\nclass Agent:\\n    def __init__(self,\\n                 goals: List[Goal],\\n                 agent_language: AgentLanguage,\\n                 action_registry: ActionRegistry,\\n                 generate_response: Callable[[Prompt], str],\\n                 environment: Environment):\\n        \"\"\"\\n        Initialize an agent with its core GAME components\\n        \"\"\"\\n        self.goals = goals\\n        self.generate_response = generate_response\\n        self.agent_language = agent_language\\n        self.actions = action_registry\\n        self.environment = environment\\n\\n    def construct_prompt(self, goals: List[Goal], memory: Memory, actions: ActionRegistry) -> Prompt:\\n        \"\"\"Build prompt with memory context\"\"\"\\n        return self.agent_language.construct_prompt(\\n            actions=actions.get_actions(),\\n            environment=self.environment,\\n            goals=goals,\\n            memory=memory\\n        )\\n\\n    def get_action(self, response):\\n        invocation = self.agent_language.parse_response(response)\\n        action = self.actions.get_action(invocation[\"tool\"])\\n        return action, invocation\\n\\n    def should_terminate(self, response: str) -> bool:\\n        action_def, _ = self.get_action(response)\\n        return action_def.terminal\\n\\n    def set_current_task(self, memory: Memory, task: str):\\n        memory.add_memory({\"type\": \"user\", \"content\": task})\\n\\n    def update_memory(self, memory: Memory, response: str, result: dict):\\n        \"\"\"\\n        Update memory with the agent\\'s decision and the environment\\'s response.\\n        \"\"\"\\n        new_memories = [\\n            {\"type\": \"assistant\", \"content\": response},\\n            {\"type\": \"environment\", \"content\": json.dumps(result)}\\n        ]\\n        for m in new_memories:\\n            memory.add_memory(m)\\n\\n    def prompt_llm_for_action(self, full_prompt: Prompt) -> str:\\n        response = self.generate_response(full_prompt)\\n        return response\\n\\n    def run(self, user_input: str, memory=None, max_iterations: int = 50) -> Memory:\\n        \"\"\"\\n        Execute the GAME loop for this agent with a maximum iteration limit.\\n        \"\"\"\\n        memory = memory or Memory()\\n        self.set_current_task(memory, user_input)\\n\\n        for _ in range(max_iterations):\\n            # Construct a prompt that includes the Goals, Actions, and the current Memory\\n            prompt = self.construct_prompt(self.goals, memory, self.actions)\\n\\n            print(\"Agent thinking...\")\\n            # Generate a response from the agent\\n            response = self.prompt_llm_for_action(prompt)\\n            print(f\"Agent Decision: {response}\")\\n\\n            # Determine which action the agent wants to execute\\n            action, invocation = self.get_action(response)\\n\\n            # Execute the action in the environment\\n            result = self.environment.execute_action(action, invocation[\"args\"])\\n            print(f\"Action Result: {result}\")\\n\\n            # Update the agent\\'s memory with information about what happened\\n            self.update_memory(memory, response, result)\\n\\n            # Check if the agent has decided to terminate\\n            if self.should_terminate(response):\\n                break\\n\\n        return memory\\n\\n\\n# ======================== YOUR CODE STARTS HERE ========================\\n\\n# Define the agent\\'s goals\\ngoals = [\\n    Goal(priority=1, name=\"Gather Information\", description=\"Read each file in the project\"),\\n    Goal(priority=1, name=\"Terminate\", description=\"Call the terminate call when you have read all the files \"\\n         \"and provide the content of the README in the terminate message\")\\n]\\n\\n# Define the agent\\'s language\\nagent_language = AgentFunctionCallingActionLanguage()\\n\\ndef read_project_file(name: str) -> str:\\n    with open(name, \"r\") as f:\\n        return f.read()\\n\\ndef list_project_files() -> List[str]:\\n    try:\\n        # Get all files (not just .py files) to see what\\'s available\\n        all_files = [f for f in os.listdir(\".\") if os.path.isfile(f)]\\n        py_files = [f for f in all_files if f.endswith(\".py\")]\\n        \\n        if not py_files:\\n            # If no .py files, return all text files\\n            text_files = [f for f in all_files if f.endswith((\\'.txt\\', \\'.md\\', \\'.json\\', \\'.yaml\\', \\'.yml\\', \\'.py\\', \\'.js\\', \\'.html\\', \\'.css\\'))]\\n            return sorted(text_files) if text_files else [\"No readable files found in current directory\"]\\n        \\n        return sorted(py_files)\\n    except Exception as e:\\n        return [f\"Error listing files: {str(e)}\"]\\n\\n# Define the action registry and register some actions\\naction_registry = ActionRegistry()\\n\\naction_registry.register(Action(\\n    name=\"list_project_files\",\\n    function=list_project_files,\\n    description=\"Lists all readable files in the project directory (prioritizing Python files, but includes other text files if no Python files exist).\",\\n    parameters={\\n        \"type\": \"object\",\\n        \"properties\": {},\\n        \"required\": []\\n    },\\n    terminal=False\\n))\\n\\naction_registry.register(Action(\\n    name=\"read_project_file\",\\n    function=read_project_file,\\n    description=\"Reads a file from the project.\",\\n    parameters={\\n        \"type\": \"object\",\\n        \"properties\": {\\n            \"name\": {\"type\": \"string\"}\\n        },\\n        \"required\": [\"name\"]\\n    },\\n    terminal=False\\n))\\n\\naction_registry.register(Action(\\n    name=\"terminate\",\\n    function=lambda message: f\"{message}\\\\nTerminating...\",\\n    description=\"Terminates the session and prints the message to the user.\",\\n    parameters={\\n        \"type\": \"object\",\\n        \"properties\": {\\n            \"message\": {\"type\": \"string\"}\\n        },\\n        \"required\": [\"message\"]\\n    },\\n    terminal=True\\n))\\n\\n# Define the environment\\nenvironment = Environment()\\n\\n# Create an agent instance\\nagent = Agent(goals, agent_language, action_registry, generate_response, environment)\\n\\n# Run the agent with user input\\nif __name__ == \"__main__\":\\n    user_input = \"Write a README for this project.\"\\n    final_memory = agent.run(user_input)\\n    \\n    # Print the final memory\\n    print(\"\\\\n\" + \"=\"*60)\\n    print(\"FINAL MEMORY:\")\\n    print(\"=\"*60)\\n    for memory in final_memory.get_memories():\\n        print(memory)\\n        print(\"-\" * 40)\\n', 'timestamp': '2025-07-24T21:10:56+0000'}\n",
            "Agent thinking...\n",
            "Agent Decision: # Cohere AI Assistant\n",
            "\n",
            "This is a Cohere AI Assistant, which is a conversational AI agent that can help you with a variety of tasks. It has been trained to assist you with your queries and requests.\n",
            "\n",
            "## How to use this assistant\n",
            "To use this assistant, simply type your request in the chat box and hit send. The assistant will respond with an answer or further questions to help clarify your request.\n",
            "\n",
            "## Available tools\n",
            "The assistant has a range of tools at its disposal to help answer your queries. These include:\n",
            "- **List project files**: Lists all readable files in the project directory, prioritising Python files but including other text files if no Python files exist.\n",
            "- **Read project file**: Reads a file from the project.\n",
            "- **Terminate**: Terminates the session and prints a message to the user.\n",
            "Action Result: {'tool_executed': True, 'result': '# Cohere AI Assistant\\n\\nThis is a Cohere AI Assistant, which is a conversational AI agent that can help you with a variety of tasks. It has been trained to assist you with your queries and requests.\\n\\n## How to use this assistant\\nTo use this assistant, simply type your request in the chat box and hit send. The assistant will respond with an answer or further questions to help clarify your request.\\n\\n## Available tools\\nThe assistant has a range of tools at its disposal to help answer your queries. These include:\\n- **List project files**: Lists all readable files in the project directory, prioritising Python files but including other text files if no Python files exist.\\n- **Read project file**: Reads a file from the project.\\n- **Terminate**: Terminates the session and prints a message to the user.\\nTerminating...', 'timestamp': '2025-07-24T21:11:07+0000'}\n",
            "\n",
            "============================================================\n",
            "FINAL MEMORY:\n",
            "============================================================\n",
            "{'type': 'user', 'content': 'Write a README for this project.'}\n",
            "----------------------------------------\n",
            "{'type': 'assistant', 'content': '{\"tool\": \"list_project_files\", \"args\": {}}'}\n",
            "----------------------------------------\n",
            "{'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": [\"main.py\"], \"timestamp\": \"2025-07-24T21:10:55+0000\"}'}\n",
            "----------------------------------------\n",
            "{'type': 'assistant', 'content': '{\"tool\": \"read_project_file\", \"args\": {\"name\": \"main.py\"}}'}\n",
            "----------------------------------------\n",
            "{'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": \"import os\\\\nfrom dataclasses import dataclass, field\\\\nfrom typing import List, Dict, Any, Callable, Optional\\\\nimport json\\\\nimport time\\\\nimport traceback\\\\nfrom langchain_cohere import ChatCohere\\\\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\\\\n\\\\n@dataclass\\\\nclass Prompt:\\\\n    messages: List[Dict] = field(default_factory=list)\\\\n    tools: List[Dict] = field(default_factory=list)\\\\n    metadata: dict = field(default_factory=dict)  # Fixing mutable default issue\\\\n\\\\n\\\\ndef generate_response(prompt: Prompt) -> str:\\\\n    \\\\\"\\\\\"\\\\\"Call LLM to get response\\\\\"\\\\\"\\\\\"\\\\n    \\\\n    # Initialize Cohere model with LangChain\\\\n    llm = ChatCohere(\\\\n        model=\\\\\"command-r-plus\\\\\",  # or \\\\\"command-r\\\\\" for faster responses\\\\n        max_tokens=1024,\\\\n        temperature=0.3\\\\n    )\\\\n\\\\n    messages = prompt.messages\\\\n    tools = prompt.tools\\\\n\\\\n    result = None\\\\n\\\\n    # Convert dict messages to LangChain message objects\\\\n    langchain_messages = []\\\\n    for msg in messages:\\\\n        if msg[\\\\\"role\\\\\"] == \\\\\"system\\\\\":\\\\n            langchain_messages.append(SystemMessage(content=msg[\\\\\"content\\\\\"]))\\\\n        elif msg[\\\\\"role\\\\\"] == \\\\\"user\\\\\":\\\\n            langchain_messages.append(HumanMessage(content=msg[\\\\\"content\\\\\"]))\\\\n        elif msg[\\\\\"role\\\\\"] == \\\\\"assistant\\\\\":\\\\n            langchain_messages.append(AIMessage(content=msg[\\\\\"content\\\\\"]))\\\\n\\\\n    if not tools:\\\\n        response = llm.invoke(langchain_messages)\\\\n        result = response.content\\\\n    else:\\\\n        # Convert tools to the format expected by LangChain Cohere\\\\n        formatted_tools = []\\\\n        for tool in tools:\\\\n            if isinstance(tool, dict) and \\\\\"function\\\\\" in tool:\\\\n                # Extract the function definition for LangChain\\\\n                func_def = tool[\\\\\"function\\\\\"]\\\\n                # Convert to Cohere-compatible format\\\\n                cohere_tool = {\\\\n                    \\\\\"title\\\\\": func_def[\\\\\"name\\\\\"],\\\\n                    \\\\\"description\\\\\": func_def[\\\\\"description\\\\\"],\\\\n                    \\\\\"properties\\\\\": func_def[\\\\\"parameters\\\\\"].get(\\\\\"properties\\\\\", {}),\\\\n                    \\\\\"required\\\\\": func_def[\\\\\"parameters\\\\\"].get(\\\\\"required\\\\\", [])\\\\n                }\\\\n                formatted_tools.append(cohere_tool)\\\\n        \\\\n        # Bind tools to the model for function calling\\\\n        llm_with_tools = llm.bind_tools(formatted_tools)\\\\n        response = llm_with_tools.invoke(langchain_messages)\\\\n\\\\n        if response.tool_calls:\\\\n            tool_call = response.tool_calls[0]\\\\n            result = {\\\\n                \\\\\"tool\\\\\": tool_call[\\\\\"name\\\\\"],\\\\n                \\\\\"args\\\\\": tool_call[\\\\\"args\\\\\"],\\\\n            }\\\\n            result = json.dumps(result)\\\\n        else:\\\\n            result = response.content\\\\n\\\\n    return result\\\\n\\\\n\\\\n@dataclass(frozen=True)\\\\nclass Goal:\\\\n    priority: int\\\\n    name: str\\\\n    description: str\\\\n\\\\n\\\\nclass Action:\\\\n    def __init__(self,\\\\n                 name: str,\\\\n                 function: Callable,\\\\n                 description: str,\\\\n                 parameters: Dict,\\\\n                 terminal: bool = False):\\\\n        self.name = name\\\\n        self.function = function\\\\n        self.description = description\\\\n        self.terminal = terminal\\\\n        self.parameters = parameters\\\\n\\\\n    def execute(self, **args) -> Any:\\\\n        \\\\\"\\\\\"\\\\\"Execute the action\\'s function\\\\\"\\\\\"\\\\\"\\\\n        return self.function(**args)\\\\n\\\\n\\\\nclass ActionRegistry:\\\\n    def __init__(self):\\\\n        self.actions = {}\\\\n\\\\n    def register(self, action: Action):\\\\n        self.actions[action.name] = action\\\\n\\\\n    def get_action(self, name: str) -> Action | None:\\\\n        return self.actions.get(name, None)\\\\n\\\\n    def get_actions(self) -> List[Action]:\\\\n        \\\\\"\\\\\"\\\\\"Get all registered actions\\\\\"\\\\\"\\\\\"\\\\n        return list(self.actions.values())\\\\n\\\\n\\\\nclass Memory:\\\\n    def __init__(self):\\\\n        self.items = []  # Basic conversation history\\\\n\\\\n    def add_memory(self, memory: dict):\\\\n        \\\\\"\\\\\"\\\\\"Add memory to working memory\\\\\"\\\\\"\\\\\"\\\\n        self.items.append(memory)\\\\n\\\\n    def get_memories(self, limit: int = None) -> List[Dict]:\\\\n        \\\\\"\\\\\"\\\\\"Get formatted conversation history for prompt\\\\\"\\\\\"\\\\\"\\\\n        return self.items[:limit]\\\\n\\\\n    def copy_without_system_memories(self):\\\\n        \\\\\"\\\\\"\\\\\"Return a copy of the memory without system memories\\\\\"\\\\\"\\\\\"\\\\n        filtered_items = [m for m in self.items if m[\\\\\"type\\\\\"] != \\\\\"system\\\\\"]\\\\n        memory = Memory()\\\\n        memory.items = filtered_items\\\\n        return memory\\\\n\\\\n\\\\nclass Environment:\\\\n    def execute_action(self, action: Action, args: dict) -> dict:\\\\n        \\\\\"\\\\\"\\\\\"Execute an action and return the result.\\\\\"\\\\\"\\\\\"\\\\n        try:\\\\n            result = action.execute(**args)\\\\n            return self.format_result(result)\\\\n        except Exception as e:\\\\n            return {\\\\n                \\\\\"tool_executed\\\\\": False,\\\\n                \\\\\"error\\\\\": str(e),\\\\n                \\\\\"traceback\\\\\": traceback.format_exc()\\\\n            }\\\\n\\\\n    def format_result(self, result: Any) -> dict:\\\\n        \\\\\"\\\\\"\\\\\"Format the result with metadata.\\\\\"\\\\\"\\\\\"\\\\n        return {\\\\n            \\\\\"tool_executed\\\\\": True,\\\\n            \\\\\"result\\\\\": result,\\\\n            \\\\\"timestamp\\\\\": time.strftime(\\\\\"%Y-%m-%dT%H:%M:%S%z\\\\\")\\\\n        }\\\\n\\\\n\\\\nclass AgentLanguage:\\\\n    def __init__(self):\\\\n        pass\\\\n\\\\n    def construct_prompt(self,\\\\n                         actions: List[Action],\\\\n                         environment: Environment,\\\\n                         goals: List[Goal],\\\\n                         memory: Memory) -> Prompt:\\\\n        raise NotImplementedError(\\\\\"Subclasses must implement this method\\\\\")\\\\n\\\\n    def parse_response(self, response: str) -> dict:\\\\n        raise NotImplementedError(\\\\\"Subclasses must implement this method\\\\\")\\\\n\\\\n\\\\nclass AgentFunctionCallingActionLanguage(AgentLanguage):\\\\n\\\\n    def __init__(self):\\\\n        super().__init__()\\\\n\\\\n    def format_goals(self, goals: List[Goal]) -> List:\\\\n        # Map all goals to a single string that concatenates their description\\\\n        # and combine into a single message of type system\\\\n        sep = \\\\\"\\\\\\\\n-------------------\\\\\\\\n\\\\\"\\\\n        goal_instructions = \\\\\"\\\\\\\\n\\\\\\\\n\\\\\".join([f\\\\\"{goal.name}:{sep}{goal.description}{sep}\\\\\" for goal in goals])\\\\n        return [\\\\n            {\\\\\"role\\\\\": \\\\\"system\\\\\", \\\\\"content\\\\\": goal_instructions}\\\\n        ]\\\\n\\\\n    def format_memory(self, memory: Memory) -> List:\\\\n        \\\\\"\\\\\"\\\\\"Generate response from language model\\\\\"\\\\\"\\\\\"\\\\n        # Map all environment results to a role:user messages\\\\n        # Map all assistant messages to a role:assistant messages\\\\n        # Map all user messages to a role:user messages\\\\n        items = memory.get_memories()\\\\n        mapped_items = []\\\\n        for item in items:\\\\n\\\\n            content = item.get(\\\\\"content\\\\\", None)\\\\n            if not content:\\\\n                content = json.dumps(item, indent=4)\\\\n\\\\n            if item[\\\\\"type\\\\\"] == \\\\\"assistant\\\\\":\\\\n                mapped_items.append({\\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": content})\\\\n            elif item[\\\\\"type\\\\\"] == \\\\\"environment\\\\\":\\\\n                # Map environment results to user messages for Cohere compatibility\\\\n                mapped_items.append({\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": f\\\\\"Tool result: {content}\\\\\"})\\\\n            else:\\\\n                mapped_items.append({\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": content})\\\\n\\\\n        return mapped_items\\\\n\\\\n    def format_actions(self, actions: List[Action]) -> List:\\\\n        \\\\\"\\\\\"\\\\\"Convert actions to LangChain-compatible tool format\\\\\"\\\\\"\\\\\"\\\\n        \\\\n        tools = []\\\\n        for action in actions:\\\\n            # Convert to OpenAI function format that LangChain can use\\\\n            tool_def = {\\\\n                \\\\\"type\\\\\": \\\\\"function\\\\\",\\\\n                \\\\\"function\\\\\": {\\\\n                    \\\\\"name\\\\\": action.name,\\\\n                    \\\\\"description\\\\\": action.description[:1024],\\\\n                    \\\\\"parameters\\\\\": action.parameters,\\\\n                }\\\\n            }\\\\n            tools.append(tool_def)\\\\n\\\\n        return tools\\\\n\\\\n    def construct_prompt(self,\\\\n                         actions: List[Action],\\\\n                         environment: Environment,\\\\n                         goals: List[Goal],\\\\n                         memory: Memory) -> Prompt:\\\\n\\\\n        prompt = []\\\\n        prompt += self.format_goals(goals)\\\\n        prompt += self.format_memory(memory)\\\\n\\\\n        tools = self.format_actions(actions)\\\\n\\\\n        return Prompt(messages=prompt, tools=tools)\\\\n\\\\n    def adapt_prompt_after_parsing_error(self,\\\\n                                         prompt: Prompt,\\\\n                                         response: str,\\\\n                                         traceback: str,\\\\n                                         error: Any,\\\\n                                         retries_left: int) -> Prompt:\\\\n\\\\n        return prompt\\\\n\\\\n    def parse_response(self, response: str) -> dict:\\\\n        \\\\\"\\\\\"\\\\\"Parse LLM response into structured format by extracting the ```json block\\\\\"\\\\\"\\\\\"\\\\n\\\\n        try:\\\\n            return json.loads(response)\\\\n\\\\n        except Exception as e:\\\\n            return {\\\\n                \\\\\"tool\\\\\": \\\\\"terminate\\\\\",\\\\n                \\\\\"args\\\\\": {\\\\\"message\\\\\": response}\\\\n            }\\\\n\\\\n\\\\nclass Agent:\\\\n    def __init__(self,\\\\n                 goals: List[Goal],\\\\n                 agent_language: AgentLanguage,\\\\n                 action_registry: ActionRegistry,\\\\n                 generate_response: Callable[[Prompt], str],\\\\n                 environment: Environment):\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        Initialize an agent with its core GAME components\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        self.goals = goals\\\\n        self.generate_response = generate_response\\\\n        self.agent_language = agent_language\\\\n        self.actions = action_registry\\\\n        self.environment = environment\\\\n\\\\n    def construct_prompt(self, goals: List[Goal], memory: Memory, actions: ActionRegistry) -> Prompt:\\\\n        \\\\\"\\\\\"\\\\\"Build prompt with memory context\\\\\"\\\\\"\\\\\"\\\\n        return self.agent_language.construct_prompt(\\\\n            actions=actions.get_actions(),\\\\n            environment=self.environment,\\\\n            goals=goals,\\\\n            memory=memory\\\\n        )\\\\n\\\\n    def get_action(self, response):\\\\n        invocation = self.agent_language.parse_response(response)\\\\n        action = self.actions.get_action(invocation[\\\\\"tool\\\\\"])\\\\n        return action, invocation\\\\n\\\\n    def should_terminate(self, response: str) -> bool:\\\\n        action_def, _ = self.get_action(response)\\\\n        return action_def.terminal\\\\n\\\\n    def set_current_task(self, memory: Memory, task: str):\\\\n        memory.add_memory({\\\\\"type\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": task})\\\\n\\\\n    def update_memory(self, memory: Memory, response: str, result: dict):\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        Update memory with the agent\\'s decision and the environment\\'s response.\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        new_memories = [\\\\n            {\\\\\"type\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": response},\\\\n            {\\\\\"type\\\\\": \\\\\"environment\\\\\", \\\\\"content\\\\\": json.dumps(result)}\\\\n        ]\\\\n        for m in new_memories:\\\\n            memory.add_memory(m)\\\\n\\\\n    def prompt_llm_for_action(self, full_prompt: Prompt) -> str:\\\\n        response = self.generate_response(full_prompt)\\\\n        return response\\\\n\\\\n    def run(self, user_input: str, memory=None, max_iterations: int = 50) -> Memory:\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        Execute the GAME loop for this agent with a maximum iteration limit.\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        memory = memory or Memory()\\\\n        self.set_current_task(memory, user_input)\\\\n\\\\n        for _ in range(max_iterations):\\\\n            # Construct a prompt that includes the Goals, Actions, and the current Memory\\\\n            prompt = self.construct_prompt(self.goals, memory, self.actions)\\\\n\\\\n            print(\\\\\"Agent thinking...\\\\\")\\\\n            # Generate a response from the agent\\\\n            response = self.prompt_llm_for_action(prompt)\\\\n            print(f\\\\\"Agent Decision: {response}\\\\\")\\\\n\\\\n            # Determine which action the agent wants to execute\\\\n            action, invocation = self.get_action(response)\\\\n\\\\n            # Execute the action in the environment\\\\n            result = self.environment.execute_action(action, invocation[\\\\\"args\\\\\"])\\\\n            print(f\\\\\"Action Result: {result}\\\\\")\\\\n\\\\n            # Update the agent\\'s memory with information about what happened\\\\n            self.update_memory(memory, response, result)\\\\n\\\\n            # Check if the agent has decided to terminate\\\\n            if self.should_terminate(response):\\\\n                break\\\\n\\\\n        return memory\\\\n\\\\n\\\\n# ======================== YOUR CODE STARTS HERE ========================\\\\n\\\\n# Define the agent\\'s goals\\\\ngoals = [\\\\n    Goal(priority=1, name=\\\\\"Gather Information\\\\\", description=\\\\\"Read each file in the project\\\\\"),\\\\n    Goal(priority=1, name=\\\\\"Terminate\\\\\", description=\\\\\"Call the terminate call when you have read all the files \\\\\"\\\\n         \\\\\"and provide the content of the README in the terminate message\\\\\")\\\\n]\\\\n\\\\n# Define the agent\\'s language\\\\nagent_language = AgentFunctionCallingActionLanguage()\\\\n\\\\ndef read_project_file(name: str) -> str:\\\\n    with open(name, \\\\\"r\\\\\") as f:\\\\n        return f.read()\\\\n\\\\ndef list_project_files() -> List[str]:\\\\n    try:\\\\n        # Get all files (not just .py files) to see what\\'s available\\\\n        all_files = [f for f in os.listdir(\\\\\".\\\\\") if os.path.isfile(f)]\\\\n        py_files = [f for f in all_files if f.endswith(\\\\\".py\\\\\")]\\\\n        \\\\n        if not py_files:\\\\n            # If no .py files, return all text files\\\\n            text_files = [f for f in all_files if f.endswith((\\'.txt\\', \\'.md\\', \\'.json\\', \\'.yaml\\', \\'.yml\\', \\'.py\\', \\'.js\\', \\'.html\\', \\'.css\\'))]\\\\n            return sorted(text_files) if text_files else [\\\\\"No readable files found in current directory\\\\\"]\\\\n        \\\\n        return sorted(py_files)\\\\n    except Exception as e:\\\\n        return [f\\\\\"Error listing files: {str(e)}\\\\\"]\\\\n\\\\n# Define the action registry and register some actions\\\\naction_registry = ActionRegistry()\\\\n\\\\naction_registry.register(Action(\\\\n    name=\\\\\"list_project_files\\\\\",\\\\n    function=list_project_files,\\\\n    description=\\\\\"Lists all readable files in the project directory (prioritizing Python files, but includes other text files if no Python files exist).\\\\\",\\\\n    parameters={\\\\n        \\\\\"type\\\\\": \\\\\"object\\\\\",\\\\n        \\\\\"properties\\\\\": {},\\\\n        \\\\\"required\\\\\": []\\\\n    },\\\\n    terminal=False\\\\n))\\\\n\\\\naction_registry.register(Action(\\\\n    name=\\\\\"read_project_file\\\\\",\\\\n    function=read_project_file,\\\\n    description=\\\\\"Reads a file from the project.\\\\\",\\\\n    parameters={\\\\n        \\\\\"type\\\\\": \\\\\"object\\\\\",\\\\n        \\\\\"properties\\\\\": {\\\\n            \\\\\"name\\\\\": {\\\\\"type\\\\\": \\\\\"string\\\\\"}\\\\n        },\\\\n        \\\\\"required\\\\\": [\\\\\"name\\\\\"]\\\\n    },\\\\n    terminal=False\\\\n))\\\\n\\\\naction_registry.register(Action(\\\\n    name=\\\\\"terminate\\\\\",\\\\n    function=lambda message: f\\\\\"{message}\\\\\\\\nTerminating...\\\\\",\\\\n    description=\\\\\"Terminates the session and prints the message to the user.\\\\\",\\\\n    parameters={\\\\n        \\\\\"type\\\\\": \\\\\"object\\\\\",\\\\n        \\\\\"properties\\\\\": {\\\\n            \\\\\"message\\\\\": {\\\\\"type\\\\\": \\\\\"string\\\\\"}\\\\n        },\\\\n        \\\\\"required\\\\\": [\\\\\"message\\\\\"]\\\\n    },\\\\n    terminal=True\\\\n))\\\\n\\\\n# Define the environment\\\\nenvironment = Environment()\\\\n\\\\n# Create an agent instance\\\\nagent = Agent(goals, agent_language, action_registry, generate_response, environment)\\\\n\\\\n# Run the agent with user input\\\\nif __name__ == \\\\\"__main__\\\\\":\\\\n    user_input = \\\\\"Write a README for this project.\\\\\"\\\\n    final_memory = agent.run(user_input)\\\\n    \\\\n    # Print the final memory\\\\n    print(\\\\\"\\\\\\\\n\\\\\" + \\\\\"=\\\\\"*60)\\\\n    print(\\\\\"FINAL MEMORY:\\\\\")\\\\n    print(\\\\\"=\\\\\"*60)\\\\n    for memory in final_memory.get_memories():\\\\n        print(memory)\\\\n        print(\\\\\"-\\\\\" * 40)\\\\n\", \"timestamp\": \"2025-07-24T21:10:56+0000\"}'}\n",
            "----------------------------------------\n",
            "{'type': 'assistant', 'content': '# Cohere AI Assistant\\n\\nThis is a Cohere AI Assistant, which is a conversational AI agent that can help you with a variety of tasks. It has been trained to assist you with your queries and requests.\\n\\n## How to use this assistant\\nTo use this assistant, simply type your request in the chat box and hit send. The assistant will respond with an answer or further questions to help clarify your request.\\n\\n## Available tools\\nThe assistant has a range of tools at its disposal to help answer your queries. These include:\\n- **List project files**: Lists all readable files in the project directory, prioritising Python files but including other text files if no Python files exist.\\n- **Read project file**: Reads a file from the project.\\n- **Terminate**: Terminates the session and prints a message to the user.'}\n",
            "----------------------------------------\n",
            "{'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": \"# Cohere AI Assistant\\\\n\\\\nThis is a Cohere AI Assistant, which is a conversational AI agent that can help you with a variety of tasks. It has been trained to assist you with your queries and requests.\\\\n\\\\n## How to use this assistant\\\\nTo use this assistant, simply type your request in the chat box and hit send. The assistant will respond with an answer or further questions to help clarify your request.\\\\n\\\\n## Available tools\\\\nThe assistant has a range of tools at its disposal to help answer your queries. These include:\\\\n- **List project files**: Lists all readable files in the project directory, prioritising Python files but including other text files if no Python files exist.\\\\n- **Read project file**: Reads a file from the project.\\\\n- **Terminate**: Terminates the session and prints a message to the user.\\\\nTerminating...\", \"timestamp\": \"2025-07-24T21:11:07+0000\"}'}\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9mpJg0nVAmwB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}