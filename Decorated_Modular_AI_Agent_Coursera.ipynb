{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5bs4iOQJmJPLDMa0fvLDG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/F-Bafti/AI-Agents-and-Agentic-AI/blob/master/Decorated_Modular_AI_Agent_Coursera.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decorated Modular AI Agent Implementation\n",
        "In our original README agent, we manually created and registered each action. While this approach works, it requires us to maintain the tool definitions (functions) separately from their metadata (descriptions and parameters). This separation creates opportunities for these two pieces to become out of sync. Letâ€™s improve our agent by using tool decorators to keep everything together and automatically synchronized."
      ],
      "metadata": {
        "id": "PpbBL3Syp6Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community\n",
        "!pip uninstall cohere langchain-cohere -y\n",
        "\n",
        "!pip install cohere>=5.0.0\n",
        "!pip install langchain-cohere\n",
        "\n",
        "!pip install cohere==5.11.0 langchain-cohere==0.3.0\n",
        "\n",
        "import json, os\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('COHERE_API_KEY')\n",
        "os.environ['COHERE_API_KEY'] = api_key"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hvoMegosp88Z"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decorated Agent Tools and Actions Setting"
      ],
      "metadata": {
        "id": "fU2ssG7ci_2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "from typing import get_type_hints, List, Callable, Dict, Any, Optional\n",
        "\n",
        "tools = {}\n",
        "tools_by_tag = {}\n",
        "\n",
        "\n",
        "def to_openai_tools(tools_metadata: List[dict]):\n",
        "    cohere_tools = [\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": t['tool_name'],\n",
        "                # Include up to 1024 characters of the description\n",
        "                \"description\": t.get('description',\"\")[:1024],\n",
        "                \"parameters\": t.get('parameters',{}),\n",
        "            },\n",
        "        } for t in tools_metadata\n",
        "    ]\n",
        "    return cohere_tools\n",
        "\n",
        "def get_tool_metadata(func, tool_name=None, description=None, parameters_override=None, terminal=False, tags=None):\n",
        "    \"\"\"\n",
        "    Extracts metadata for a function to use in tool registration.\n",
        "\n",
        "    Parameters:\n",
        "        func (function): The function to extract metadata from.\n",
        "        tool_name (str, optional): The name of the tool. Defaults to the function name.\n",
        "        description (str, optional): Description of the tool. Defaults to the function's docstring.\n",
        "        parameters_override (dict, optional): Override for the argument schema. Defaults to dynamically inferred schema.\n",
        "        terminal (bool, optional): Whether the tool is terminal. Defaults to False.\n",
        "        tags (List[str], optional): List of tags to associate with the tool.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing metadata about the tool, including description, args schema, and the function.\n",
        "    \"\"\"\n",
        "    # Default tool_name to the function name if not provided\n",
        "    tool_name = tool_name or func.__name__\n",
        "\n",
        "    # Default description to the function's docstring if not provided\n",
        "    description = description or (func.__doc__.strip() if func.__doc__ else \"No description provided.\")\n",
        "\n",
        "    # Discover the function's signature and type hints if no args_override is provided\n",
        "    if parameters_override is None:\n",
        "        signature = inspect.signature(func)\n",
        "        type_hints = get_type_hints(func)\n",
        "\n",
        "        # Build the arguments schema dynamically\n",
        "        args_schema = {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {},\n",
        "            \"required\": []\n",
        "        }\n",
        "        for param_name, param in signature.parameters.items():\n",
        "\n",
        "            if param_name in [\"action_context\", \"action_agent\"]:\n",
        "                continue  # Skip these parameters\n",
        "\n",
        "            def get_json_type(param_type):\n",
        "                if param_type == str:\n",
        "                    return \"string\"\n",
        "                elif param_type == int:\n",
        "                    return \"integer\"\n",
        "                elif param_type == float:\n",
        "                    return \"number\"\n",
        "                elif param_type == bool:\n",
        "                    return \"boolean\"\n",
        "                elif param_type == list:\n",
        "                    return \"array\"\n",
        "                elif param_type == dict:\n",
        "                    return \"object\"\n",
        "                else:\n",
        "                    return \"string\"\n",
        "\n",
        "            # Add parameter details\n",
        "            param_type = type_hints.get(param_name, str)  # Default to string if type is not annotated\n",
        "            param_schema = {\"type\": get_json_type(param_type)}  # Convert Python types to JSON schema types\n",
        "\n",
        "            args_schema[\"properties\"][param_name] = param_schema\n",
        "\n",
        "            # Add to required if not defaulted\n",
        "            if param.default == inspect.Parameter.empty:\n",
        "                args_schema[\"required\"].append(param_name)\n",
        "    else:\n",
        "        args_schema = parameters_override\n",
        "\n",
        "    # Return the metadata as a dictionary\n",
        "    return {\n",
        "        \"tool_name\": tool_name,\n",
        "        \"description\": description,\n",
        "        \"parameters\": args_schema,\n",
        "        \"function\": func,\n",
        "        \"terminal\": terminal,\n",
        "        \"tags\": tags or []\n",
        "    }\n",
        "\n",
        "\n",
        "def register_tool(tool_name=None, description=None, parameters_override=None, terminal=False, tags=None):\n",
        "    \"\"\"\n",
        "    A decorator to dynamically register a function in the tools dictionary with its parameters, schema, and docstring.\n",
        "\n",
        "    Parameters:\n",
        "        tool_name (str, optional): The name of the tool to register. Defaults to the function name.\n",
        "        description (str, optional): Override for the tool's description. Defaults to the function's docstring.\n",
        "        parameters_override (dict, optional): Override for the argument schema. Defaults to dynamically inferred schema.\n",
        "        terminal (bool, optional): Whether the tool is terminal. Defaults to False.\n",
        "        tags (List[str], optional): List of tags to associate with the tool.\n",
        "\n",
        "    Returns:\n",
        "        function: The wrapped function.\n",
        "    \"\"\"\n",
        "    def decorator(func):\n",
        "        # Use the reusable function to extract metadata\n",
        "        metadata = get_tool_metadata(\n",
        "            func=func,\n",
        "            tool_name=tool_name,\n",
        "            description=description,\n",
        "            parameters_override=parameters_override,\n",
        "            terminal=terminal,\n",
        "            tags=tags\n",
        "        )\n",
        "\n",
        "        # Register the tool in the global dictionary\n",
        "        tools[metadata[\"tool_name\"]] = {\n",
        "            \"description\": metadata[\"description\"],\n",
        "            \"parameters\": metadata[\"parameters\"],\n",
        "            \"function\": metadata[\"function\"],\n",
        "            \"terminal\": metadata[\"terminal\"],\n",
        "            \"tags\": metadata[\"tags\"] or []\n",
        "        }\n",
        "\n",
        "        for tag in metadata[\"tags\"]:\n",
        "            if tag not in tools_by_tag:\n",
        "                tools_by_tag[tag] = []\n",
        "            tools_by_tag[tag].append(metadata[\"tool_name\"])\n",
        "\n",
        "        return func\n",
        "    return decorator\n"
      ],
      "metadata": {
        "id": "lcb7lvX1jJi9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Prompt Class and Agent Response"
      ],
      "metadata": {
        "id": "FKWqNkoWApOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "import json\n",
        "import time\n",
        "import asyncio\n",
        "import traceback\n",
        "from langchain_cohere import ChatCohere\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError\n",
        "\n",
        "@dataclass\n",
        "class Prompt:\n",
        "    messages: List[Dict] = field(default_factory=list)\n",
        "    tools: List[Dict] = field(default_factory=list)\n",
        "    metadata: dict = field(default_factory=dict)  # Fixing mutable default issue\n",
        "\n",
        "\n",
        "\n",
        "def generate_response(prompt: Prompt) -> str:\n",
        "    \"\"\"Call LLM to get response\"\"\"\n",
        "    print(\"DEBUG: Starting generate_response\")\n",
        "    print(f\"DEBUG: Prompt has {len(prompt.messages)} messages and {len(prompt.tools)} tools\")\n",
        "\n",
        "    try:\n",
        "        # Initialize Cohere model with LangChain\n",
        "        llm = ChatCohere(\n",
        "            model=\"command-r-plus\",\n",
        "            max_tokens=1024,\n",
        "            temperature=0.3,\n",
        "            timeout=30  # Add timeout\n",
        "        )\n",
        "        print(\"DEBUG: Cohere model initialized\")\n",
        "\n",
        "        messages = prompt.messages\n",
        "        tools = prompt.tools\n",
        "        result = None\n",
        "\n",
        "        # Convert dict messages to LangChain message objects\n",
        "        langchain_messages = []\n",
        "        for msg in messages:\n",
        "            if msg[\"role\"] == \"system\":\n",
        "                langchain_messages.append(SystemMessage(content=msg[\"content\"]))\n",
        "            elif msg[\"role\"] == \"user\":\n",
        "                langchain_messages.append(HumanMessage(content=msg[\"content\"]))\n",
        "            elif msg[\"role\"] == \"assistant\":\n",
        "                langchain_messages.append(AIMessage(content=msg[\"content\"]))\n",
        "\n",
        "        print(f\"DEBUG: Converted {len(langchain_messages)} messages\")\n",
        "\n",
        "        # If there is no tools for LLM to use just get the text response\n",
        "        if not tools:\n",
        "            print(\"DEBUG: No tools, making simple invoke call\")\n",
        "            response = llm.invoke(langchain_messages)\n",
        "            result = response.content\n",
        "        else:\n",
        "            print(\"DEBUG: Tools present, setting up tool calling\")\n",
        "\n",
        "            # Debug: Print the tools before formatting\n",
        "            print(f\"DEBUG: Original tools: {tools}\")\n",
        "\n",
        "            formatted_tools = []\n",
        "            for tool in tools:\n",
        "                if isinstance(tool, dict) and \"function\" in tool:\n",
        "                    func_def = tool[\"function\"]\n",
        "                    cohere_tool = {\n",
        "                        \"title\": func_def[\"name\"],\n",
        "                        \"description\": func_def[\"description\"],\n",
        "                        \"properties\": func_def[\"parameters\"].get(\"properties\", {}),\n",
        "                        \"required\": func_def[\"parameters\"].get(\"required\", [])\n",
        "                    }\n",
        "                    formatted_tools.append(cohere_tool)\n",
        "\n",
        "            print(f\"DEBUG: Formatted tools: {formatted_tools}\")\n",
        "\n",
        "            llm_with_tools = llm.bind_tools(formatted_tools)\n",
        "            print(\"DEBUG: About to invoke LLM with tools (this may take 30-60 seconds)\")\n",
        "\n",
        "            # Add timeout wrapper\n",
        "            def make_call():\n",
        "                return llm_with_tools.invoke(langchain_messages)\n",
        "\n",
        "            with ThreadPoolExecutor() as executor:\n",
        "                future = executor.submit(make_call)\n",
        "                try:\n",
        "                    response = future.result(timeout=60)  # 60 second timeout\n",
        "                    print(\"DEBUG: Got response from LLM with tools\")\n",
        "                except FuturesTimeoutError:\n",
        "                    print(\"DEBUG: LLM call timed out after 60 seconds\")\n",
        "                    return \"ERROR: LLM call timed out\"\n",
        "\n",
        "            if response.tool_calls:\n",
        "                tool_call = response.tool_calls[0]\n",
        "                result = {\n",
        "                    \"tool\": tool_call[\"name\"],\n",
        "                    \"args\": tool_call[\"args\"],\n",
        "                }\n",
        "                result = json.dumps(result)\n",
        "            else:\n",
        "                result = response.content\n",
        "\n",
        "        print(f\"DEBUG: Returning result: {result}\")\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"DEBUG: Exception occurred: {type(e).__name__}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return f\"ERROR: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "pyCk9AJy2QXE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up the GAME(Goal, Action, Memory, Environment)"
      ],
      "metadata": {
        "id": "rJwRLroIB3Us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1- Goal Class\n",
        "# Creates a simple container to hold information about what the agent should accomplish\n",
        "# priority: How important this goal is (lower numbers = higher priority)\n",
        "# name: Short name for the goal (like \"Gather Information\")\n",
        "# description: Detailed explanation of what to do\n",
        "# frozen=True: Makes this immutable - once created, it can't be changed\n",
        "@dataclass(frozen=True)\n",
        "class Goal:\n",
        "    priority: int\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "\n",
        "\n",
        "# 2- Action Class\n",
        "# Creates a wrapper around a Python function to make it available to the AI\n",
        "# name: What the AI will call this action (like \"read_file\")\n",
        "# function: The actual Python function to execute\n",
        "# description: Tells the AI what this function does\n",
        "# parameters: Describes what arguments the function needs (JSON schema format)\n",
        "# terminal: Whether calling this action should end the agent's execution\n",
        "class Action:\n",
        "    def __init__(self,\n",
        "                 name: str,\n",
        "                 function: Callable,\n",
        "                 description: str,\n",
        "                 parameters: Dict,\n",
        "                 terminal: bool = False):\n",
        "        self.name = name\n",
        "        self.function = function\n",
        "        self.description = description\n",
        "        self.terminal = terminal\n",
        "        self.parameters = parameters\n",
        "\n",
        "    def execute(self, **args) -> Any:\n",
        "        \"\"\"Execute the action's function\"\"\"\n",
        "        return self.function(**args)\n",
        "\n",
        "# Creates a container to store all available actions\n",
        "class ActionRegistry:\n",
        "    def __init__(self):\n",
        "        self.actions = {}\n",
        "\n",
        "    def register(self, action: Action):\n",
        "        self.actions[action.name] = action\n",
        "\n",
        "    # Looks up an action by its name\n",
        "    def get_action(self, name: str) -> Action | None:\n",
        "        return self.actions.get(name, None)\n",
        "\n",
        "    # Returns ALL actions as a list\n",
        "    def get_actions(self) -> List[Action]:\n",
        "        \"\"\"Get all registered actions\"\"\"\n",
        "        return list(self.actions.values())\n",
        "\n",
        "\n",
        "\n",
        "# 3- Memory Class\n",
        "# Creates a container to store the conversation history\n",
        "# self.items = []: An empty list to hold memory items\n",
        "# Each item will be a dictionary representing one piece of the conversation\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.items = []  # Basic conversation history\n",
        "\n",
        "    # Adds a new memory item to the end of the list\n",
        "    def add_memory(self, memory: dict):\n",
        "        \"\"\"Add memory to working memory\"\"\"\n",
        "        self.items.append(memory)\n",
        "\n",
        "    # Returns the stored memories as a list\n",
        "    def get_memories(self, limit: int = None) -> List[Dict]:\n",
        "        \"\"\"Get formatted conversation history for prompt\"\"\"\n",
        "        return self.items[:limit]\n",
        "\n",
        "    # Creates a new Memory object with system messages filtered out\n",
        "    def copy_without_system_memories(self):\n",
        "        \"\"\"Return a copy of the memory without system memories\"\"\"\n",
        "        filtered_items = [m for m in self.items if m[\"type\"] != \"system\"]\n",
        "        memory = Memory()\n",
        "        memory.items = filtered_items\n",
        "        return memory\n",
        "\n",
        "\n",
        "\n",
        "# 4- Environment Class\n",
        "# This is where actions actually get executed safely\n",
        "# try: attempts to run the action\n",
        "# action.execute(**args) calls the action with the provided arguments\n",
        "# If it works: calls self.format_result() to package the result nicely\n",
        "# If it fails: catches the error and returns error information instead of crashing\n",
        "class Environment:\n",
        "    def execute_action(self, action: Action, args: dict) -> dict:\n",
        "        \"\"\"Execute an action and return the result.\"\"\"\n",
        "        try:\n",
        "            result = action.execute(**args)\n",
        "            return self.format_result(result)\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"tool_executed\": False,\n",
        "                \"error\": str(e),\n",
        "                \"traceback\": traceback.format_exc()\n",
        "            }\n",
        "\n",
        "    def format_result(self, result: Any) -> dict:\n",
        "        \"\"\"Format the result with metadata.\"\"\"\n",
        "        return {\n",
        "            \"tool_executed\": True,\n",
        "            \"result\": result,\n",
        "            \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
        "        }\n"
      ],
      "metadata": {
        "id": "oKY-KLx8B2pW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the Agent Language and Function-Calling"
      ],
      "metadata": {
        "id": "7C5YsptXHFyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What this class do?\n",
        "# Example: user_input = \"Write a README for this project.\"\n",
        "# What construct_prompt Does:\n",
        "# The agent takes that simple user input and builds a complex prompt that includes:\n",
        "# System instructions (the goals), Conversation history, Available tools, The user's request\n",
        "# Prompt(\n",
        "#     messages=[\n",
        "#         {\"role\": \"system\", \"content\": \"Goal: Read each file...\"},\n",
        "#         {\"role\": \"user\", \"content\": \"Write a README for this project.\"},\n",
        "#         {\"role\": \"assistant\", \"content\": '{\"tool\": \"list_files\", \"args\": {}}'},\n",
        "#         {\"role\": \"user\", \"content\": \"Tool result: [file1.py, file2.py]\"}\n",
        "#     ],\n",
        "#     tools=[list_files_tool, read_file_tool, terminate_tool]\n",
        "# )\n",
        "\n",
        "class AgentLanguage:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def construct_prompt(self,\n",
        "                         actions: List[Action],\n",
        "                         environment: Environment,\n",
        "                         goals: List[Goal],\n",
        "                         memory: Memory) -> Prompt:\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "    def parse_response(self, response: str) -> dict:\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "\n",
        "\n",
        "# This is a concrete implementation of the abstract AgentLanguage class\n",
        "class AgentFunctionCallingActionLanguage(AgentLanguage):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # Takes a list of Goal objects and converts them into a system message\n",
        "    # Creates a formatted string with separators to make it readable\n",
        "    # Returns a list with one system message containing all goals\n",
        "    def format_goals(self, goals: List[Goal]) -> List:\n",
        "        # Map all goals to a single string that concatenates their description\n",
        "        # and combine into a single message of type system\n",
        "        sep = \"\\n-------------------\\n\"\n",
        "        goal_instructions = \"\\n\\n\".join([f\"{goal.name}:{sep}{goal.description}{sep}\" for goal in goals])\n",
        "        return [\n",
        "            {\"role\": \"system\", \"content\": goal_instructions}\n",
        "        ]\n",
        "\n",
        "\n",
        "    def format_memory(self, memory: Memory) -> List:\n",
        "        \"\"\"Generate response from language model\"\"\"\n",
        "        # Map all environment results to a role:user messages\n",
        "        # Map all assistant messages to a role:assistant messages\n",
        "        # Map all user messages to a role:user messages\n",
        "        items = memory.get_memories()\n",
        "        mapped_items = []\n",
        "        for item in items:\n",
        "\n",
        "            content = item.get(\"content\", None)\n",
        "            if not content:\n",
        "                content = json.dumps(item, indent=4)\n",
        "\n",
        "            if item[\"type\"] == \"assistant\":\n",
        "                mapped_items.append({\"role\": \"assistant\", \"content\": content})\n",
        "            elif item[\"type\"] == \"environment\":\n",
        "                # Map environment results to user messages for Cohere compatibility\n",
        "                mapped_items.append({\"role\": \"user\", \"content\": f\"Tool result: {content}\"})\n",
        "            else:\n",
        "                mapped_items.append({\"role\": \"user\", \"content\": content})\n",
        "\n",
        "        return mapped_items\n",
        "\n",
        "    def format_actions(self, actions: List[Action]) -> List:\n",
        "        \"\"\"Convert actions to LangChain-compatible tool format\"\"\"\n",
        "\n",
        "        tools = []\n",
        "        for action in actions:\n",
        "            # Convert to OpenAI function format that LangChain can use\n",
        "            tool_def = {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": action.name,\n",
        "                    \"description\": action.description[:1024],\n",
        "                    \"parameters\": action.parameters,\n",
        "                }\n",
        "            }\n",
        "            tools.append(tool_def)\n",
        "\n",
        "        return tools\n",
        "\n",
        "    def construct_prompt(self,\n",
        "                         actions: List[Action],\n",
        "                         environment: Environment,\n",
        "                         goals: List[Goal],\n",
        "                         memory: Memory) -> Prompt:\n",
        "\n",
        "        prompt = []\n",
        "        prompt += self.format_goals(goals)\n",
        "        prompt += self.format_memory(memory)\n",
        "\n",
        "        tools = self.format_actions(actions)\n",
        "\n",
        "        return Prompt(messages=prompt, tools=tools)\n",
        "\n",
        "    def adapt_prompt_after_parsing_error(self,\n",
        "                                         prompt: Prompt,\n",
        "                                         response: str,\n",
        "                                         traceback: str,\n",
        "                                         error: Any,\n",
        "                                         retries_left: int) -> Prompt:\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def parse_response(self, response: str) -> dict:\n",
        "        \"\"\"Parse LLM response into structured format by extracting the ```json block\"\"\"\n",
        "\n",
        "        try:\n",
        "            return json.loads(response)\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"tool\": \"terminate\",\n",
        "                \"args\": {\"message\": response}\n",
        "            }\n",
        "\n",
        "\n",
        "class PythonActionRegistry(ActionRegistry):\n",
        "    def __init__(self, tags: List[str] = None, tool_names: List[str] = None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.terminate_tool = None\n",
        "\n",
        "        for tool_name, tool_desc in tools.items():\n",
        "            if tool_name == \"terminate\":\n",
        "                self.terminate_tool = tool_desc\n",
        "\n",
        "            if tool_names and tool_name not in tool_names:\n",
        "                continue\n",
        "\n",
        "            tool_tags = tool_desc.get(\"tags\", [])\n",
        "            if tags and not any(tag in tool_tags for tag in tags):\n",
        "                continue\n",
        "\n",
        "            self.register(Action(\n",
        "                name=tool_name,\n",
        "                function=tool_desc[\"function\"],\n",
        "                description=tool_desc[\"description\"],\n",
        "                parameters=tool_desc.get(\"parameters\", {}),\n",
        "                terminal=tool_desc.get(\"terminal\", False)\n",
        "            ))\n",
        "\n",
        "    def register_terminate_tool(self):\n",
        "        if self.terminate_tool:\n",
        "            self.register(Action(\n",
        "                name=\"terminate\",\n",
        "                function=self.terminate_tool[\"function\"],\n",
        "                description=self.terminate_tool[\"description\"],\n",
        "                parameters=self.terminate_tool.get(\"parameters\", {}),\n",
        "                terminal=self.terminate_tool.get(\"terminal\", False)\n",
        "            ))\n",
        "        else:\n",
        "            raise Exception(\"Terminate tool not found in tool registry\")\n"
      ],
      "metadata": {
        "id": "VIfRARNp2RfO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Agent Class"
      ],
      "metadata": {
        "id": "y4eONEZiOFsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self,\n",
        "                 goals: List[Goal],\n",
        "                 agent_language: AgentLanguage,\n",
        "                 action_registry: ActionRegistry,\n",
        "                 generate_response: Callable[[Prompt], str],\n",
        "                 environment: Environment):\n",
        "        \"\"\"\n",
        "        Initialize an agent with its core GAME components\n",
        "        \"\"\"\n",
        "        self.goals = goals\n",
        "        self.generate_response = generate_response\n",
        "        self.agent_language = agent_language\n",
        "        self.actions = action_registry\n",
        "        self.environment = environment\n",
        "\n",
        "    def construct_prompt(self, goals: List[Goal], memory: Memory, actions: ActionRegistry) -> Prompt:\n",
        "        \"\"\"Build prompt with memory context\"\"\"\n",
        "        return self.agent_language.construct_prompt(\n",
        "            actions=actions.get_actions(),\n",
        "            environment=self.environment,\n",
        "            goals=goals,\n",
        "            memory=memory\n",
        "        )\n",
        "\n",
        "    def get_action(self, response):\n",
        "        invocation = self.agent_language.parse_response(response)\n",
        "        action = self.actions.get_action(invocation[\"tool\"])\n",
        "        return action, invocation\n",
        "\n",
        "    def should_terminate(self, response: str) -> bool:\n",
        "        action_def, _ = self.get_action(response)\n",
        "        return action_def.terminal\n",
        "\n",
        "    def set_current_task(self, memory: Memory, task: str):\n",
        "        memory.add_memory({\"type\": \"user\", \"content\": task})\n",
        "\n",
        "    def update_memory(self, memory: Memory, response: str, result: dict):\n",
        "        \"\"\"\n",
        "        Update memory with the agent's decision and the environment's response.\n",
        "        \"\"\"\n",
        "        new_memories = [\n",
        "            {\"type\": \"assistant\", \"content\": response},\n",
        "            {\"type\": \"environment\", \"content\": json.dumps(result)}\n",
        "        ]\n",
        "        for m in new_memories:\n",
        "            memory.add_memory(m)\n",
        "\n",
        "    def prompt_llm_for_action(self, full_prompt: Prompt) -> str:\n",
        "        response = self.generate_response(full_prompt)\n",
        "        return response\n",
        "\n",
        "    def run(self, user_input: str, memory=None, max_iterations: int = 50) -> Memory:\n",
        "        \"\"\"\n",
        "        Execute the GAME loop for this agent with a maximum iteration limit.\n",
        "        \"\"\"\n",
        "        memory = memory or Memory()\n",
        "        self.set_current_task(memory, user_input)\n",
        "\n",
        "        for _ in range(max_iterations):\n",
        "            # Construct a prompt that includes the Goals, Actions, and the current Memory\n",
        "            prompt = self.construct_prompt(self.goals, memory, self.actions)\n",
        "\n",
        "            print(\"Agent thinking...\")\n",
        "            # Generate a response from the agent\n",
        "            response = self.prompt_llm_for_action(prompt)\n",
        "            print(f\"Agent Decision: {response}\")\n",
        "\n",
        "            # Determine which action the agent wants to execute\n",
        "            action, invocation = self.get_action(response)\n",
        "\n",
        "            # Execute the action in the environment\n",
        "            result = self.environment.execute_action(action, invocation[\"args\"])\n",
        "            print(f\"Action Result: {result}\")\n",
        "\n",
        "            # Update the agent's memory with information about what happened\n",
        "            self.update_memory(memory, response, result)\n",
        "\n",
        "            # Check if the agent has decided to terminate\n",
        "            if self.should_terminate(response):\n",
        "                break\n",
        "\n",
        "        return memory"
      ],
      "metadata": {
        "id": "Y7QZrPaoNRpk"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Code"
      ],
      "metadata": {
        "id": "wc5S2XMTRAm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we'll define our tools using decorators\n",
        "@register_tool(tags=[\"file_operations\", \"read\"])\n",
        "def read_project_file(name: str) -> str:\n",
        "    \"\"\"Reads and returns the content of a specified project file.\n",
        "\n",
        "    Opens the file in read mode and returns its entire contents as a string.\n",
        "    Raises FileNotFoundError if the file doesn't exist.\n",
        "\n",
        "    Args:\n",
        "        name: The name of the file to read\n",
        "\n",
        "    Returns:\n",
        "        The contents of the file as a string\n",
        "    \"\"\"\n",
        "    with open(name, \"r\") as f:\n",
        "        return f.read()\n",
        "\n",
        "@register_tool(tags=[\"file_operations\", \"list\"])\n",
        "def list_project_files() -> List[str]:\n",
        "    \"\"\"Lists all Python files in the current project directory.\n",
        "\n",
        "    Scans the current directory and returns a sorted list of all files\n",
        "    that end with '.py'.\n",
        "\n",
        "    Returns:\n",
        "        A sorted list of Python filenames\n",
        "    \"\"\"\n",
        "    return sorted([file for file in os.listdir(\".\")\n",
        "                  if file.endswith(\".py\")])\n",
        "\n",
        "@register_tool(tags=[\"system\"], terminal=True)\n",
        "def terminate(message: str) -> str:\n",
        "    \"\"\"Terminates the agent's execution with a final message.\n",
        "\n",
        "    Args:\n",
        "        message: The final message to return before terminating\n",
        "\n",
        "    Returns:\n",
        "        The message with a termination note appended\n",
        "    \"\"\"\n",
        "    return f\"{message}\\nTerminating...\"\n",
        "\n",
        "\n",
        "\n",
        "# Define the agent's goals\n",
        "goals = [\n",
        "    Goal(priority=1,\n",
        "          name=\"Gather Information\",\n",
        "          description=\"Read each file in the project in order to build a deep understanding of the project in order to write a README\"),\n",
        "    Goal(priority=1,\n",
        "          name=\"Terminate\",\n",
        "          description=\"Call terminate when done and provide a complete README for the project in the message parameter\")\n",
        "]\n",
        "\n",
        "# Create an agent instance with tag-filtered actions\n",
        "\n",
        "agent = Agent(\n",
        "    goals=goals,\n",
        "    agent_language=AgentFunctionCallingActionLanguage(),\n",
        "    # The ActionRegistry now automatically loads tools with these tags\n",
        "    action_registry=PythonActionRegistry(tags=[\"file_operations\", \"system\"]),\n",
        "    generate_response=generate_response,\n",
        "    environment=Environment()\n",
        ")\n",
        "\n",
        "# Run the agent with user input\n",
        "user_input = \"Write a README for this project.\"\n",
        "final_memory = agent.run(user_input)\n",
        "print(final_memory.get_memories())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l38Eto2YNWAT",
        "outputId": "5c1a1239-1d78-43fb-8732-677777f231c5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent thinking...\n",
            "DEBUG: Starting generate_response\n",
            "DEBUG: Prompt has 2 messages and 3 tools\n",
            "DEBUG: Cohere model initialized\n",
            "DEBUG: Converted 2 messages\n",
            "DEBUG: Tools present, setting up tool calling\n",
            "DEBUG: Original tools: [{'type': 'function', 'function': {'name': 'read_project_file', 'description': \"Reads and returns the content of a specified project file.\\n\\n    Opens the file in read mode and returns its entire contents as a string.\\n    Raises FileNotFoundError if the file doesn't exist.\\n\\n    Args:\\n        name: The name of the file to read\\n\\n    Returns:\\n        The contents of the file as a string\", 'parameters': {'type': 'object', 'properties': {'name': {'type': 'string'}}, 'required': ['name']}}}, {'type': 'function', 'function': {'name': 'list_project_files', 'description': \"Lists all Python files in the current project directory.\\n\\n    Scans the current directory and returns a sorted list of all files\\n    that end with '.py'.\\n\\n    Returns:\\n        A sorted list of Python filenames\", 'parameters': {'type': 'object', 'properties': {}, 'required': []}}}, {'type': 'function', 'function': {'name': 'terminate', 'description': \"Terminates the agent's execution with a final message.\\n\\n    Args:\\n        message: The final message to return before terminating\\n\\n    Returns:\\n        The message with a termination note appended\", 'parameters': {'type': 'object', 'properties': {'message': {'type': 'string'}}, 'required': ['message']}}}]\n",
            "DEBUG: Formatted tools: [{'title': 'read_project_file', 'description': \"Reads and returns the content of a specified project file.\\n\\n    Opens the file in read mode and returns its entire contents as a string.\\n    Raises FileNotFoundError if the file doesn't exist.\\n\\n    Args:\\n        name: The name of the file to read\\n\\n    Returns:\\n        The contents of the file as a string\", 'properties': {'name': {'type': 'string'}}, 'required': ['name']}, {'title': 'list_project_files', 'description': \"Lists all Python files in the current project directory.\\n\\n    Scans the current directory and returns a sorted list of all files\\n    that end with '.py'.\\n\\n    Returns:\\n        A sorted list of Python filenames\", 'properties': {}, 'required': []}, {'title': 'terminate', 'description': \"Terminates the agent's execution with a final message.\\n\\n    Args:\\n        message: The final message to return before terminating\\n\\n    Returns:\\n        The message with a termination note appended\", 'properties': {'message': {'type': 'string'}}, 'required': ['message']}]\n",
            "DEBUG: About to invoke LLM with tools (this may take 30-60 seconds)\n",
            "DEBUG: Got response from LLM with tools\n",
            "DEBUG: Returning result: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['main.py'], 'timestamp': '2025-07-25T18:06:57+0000'}\n",
            "Agent thinking...\n",
            "DEBUG: Starting generate_response\n",
            "DEBUG: Prompt has 4 messages and 3 tools\n",
            "DEBUG: Cohere model initialized\n",
            "DEBUG: Converted 4 messages\n",
            "DEBUG: Tools present, setting up tool calling\n",
            "DEBUG: Original tools: [{'type': 'function', 'function': {'name': 'read_project_file', 'description': \"Reads and returns the content of a specified project file.\\n\\n    Opens the file in read mode and returns its entire contents as a string.\\n    Raises FileNotFoundError if the file doesn't exist.\\n\\n    Args:\\n        name: The name of the file to read\\n\\n    Returns:\\n        The contents of the file as a string\", 'parameters': {'type': 'object', 'properties': {'name': {'type': 'string'}}, 'required': ['name']}}}, {'type': 'function', 'function': {'name': 'list_project_files', 'description': \"Lists all Python files in the current project directory.\\n\\n    Scans the current directory and returns a sorted list of all files\\n    that end with '.py'.\\n\\n    Returns:\\n        A sorted list of Python filenames\", 'parameters': {'type': 'object', 'properties': {}, 'required': []}}}, {'type': 'function', 'function': {'name': 'terminate', 'description': \"Terminates the agent's execution with a final message.\\n\\n    Args:\\n        message: The final message to return before terminating\\n\\n    Returns:\\n        The message with a termination note appended\", 'parameters': {'type': 'object', 'properties': {'message': {'type': 'string'}}, 'required': ['message']}}}]\n",
            "DEBUG: Formatted tools: [{'title': 'read_project_file', 'description': \"Reads and returns the content of a specified project file.\\n\\n    Opens the file in read mode and returns its entire contents as a string.\\n    Raises FileNotFoundError if the file doesn't exist.\\n\\n    Args:\\n        name: The name of the file to read\\n\\n    Returns:\\n        The contents of the file as a string\", 'properties': {'name': {'type': 'string'}}, 'required': ['name']}, {'title': 'list_project_files', 'description': \"Lists all Python files in the current project directory.\\n\\n    Scans the current directory and returns a sorted list of all files\\n    that end with '.py'.\\n\\n    Returns:\\n        A sorted list of Python filenames\", 'properties': {}, 'required': []}, {'title': 'terminate', 'description': \"Terminates the agent's execution with a final message.\\n\\n    Args:\\n        message: The final message to return before terminating\\n\\n    Returns:\\n        The message with a termination note appended\", 'properties': {'message': {'type': 'string'}}, 'required': ['message']}]\n",
            "DEBUG: About to invoke LLM with tools (this may take 30-60 seconds)\n",
            "DEBUG: Got response from LLM with tools\n",
            "DEBUG: Returning result: {\"tool\": \"read_project_file\", \"args\": {\"name\": \"main.py\"}}\n",
            "Agent Decision: {\"tool\": \"read_project_file\", \"args\": {\"name\": \"main.py\"}}\n",
            "Action Result: {'tool_executed': True, 'result': 'import os\\nfrom dataclasses import dataclass, field\\nfrom typing import List, Dict, Any, Callable, Optional\\nimport json\\nimport time\\nimport traceback\\nfrom langchain_cohere import ChatCohere\\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\\n\\n@dataclass\\nclass Prompt:\\n    messages: List[Dict] = field(default_factory=list)\\n    tools: List[Dict] = field(default_factory=list)\\n    metadata: dict = field(default_factory=dict)  # Fixing mutable default issue\\n\\n\\ndef generate_response(prompt: Prompt) -> str:\\n    \"\"\"Call LLM to get response\"\"\"\\n    \\n    # Initialize Cohere model with LangChain\\n    llm = ChatCohere(\\n        model=\"command-r-plus\",  # or \"command-r\" for faster responses\\n        max_tokens=1024,\\n        temperature=0.3\\n    )\\n\\n    messages = prompt.messages\\n    tools = prompt.tools\\n\\n    result = None\\n\\n    # Convert dict messages to LangChain message objects\\n    langchain_messages = []\\n    for msg in messages:\\n        if msg[\"role\"] == \"system\":\\n            langchain_messages.append(SystemMessage(content=msg[\"content\"]))\\n        elif msg[\"role\"] == \"user\":\\n            langchain_messages.append(HumanMessage(content=msg[\"content\"]))\\n        elif msg[\"role\"] == \"assistant\":\\n            langchain_messages.append(AIMessage(content=msg[\"content\"]))\\n\\n    if not tools:\\n        response = llm.invoke(langchain_messages)\\n        result = response.content\\n    else:\\n        # Convert tools to the format expected by LangChain Cohere\\n        formatted_tools = []\\n        for tool in tools:\\n            if isinstance(tool, dict) and \"function\" in tool:\\n                # Extract the function definition for LangChain\\n                func_def = tool[\"function\"]\\n                # Convert to Cohere-compatible format\\n                cohere_tool = {\\n                    \"title\": func_def[\"name\"],\\n                    \"description\": func_def[\"description\"],\\n                    \"properties\": func_def[\"parameters\"].get(\"properties\", {}),\\n                    \"required\": func_def[\"parameters\"].get(\"required\", [])\\n                }\\n                formatted_tools.append(cohere_tool)\\n        \\n        # Bind tools to the model for function calling\\n        llm_with_tools = llm.bind_tools(formatted_tools)\\n        response = llm_with_tools.invoke(langchain_messages)\\n\\n        if response.tool_calls:\\n            tool_call = response.tool_calls[0]\\n            result = {\\n                \"tool\": tool_call[\"name\"],\\n                \"args\": tool_call[\"args\"],\\n            }\\n            result = json.dumps(result)\\n        else:\\n            result = response.content\\n\\n    return result\\n\\n\\n@dataclass(frozen=True)\\nclass Goal:\\n    priority: int\\n    name: str\\n    description: str\\n\\n\\nclass Action:\\n    def __init__(self,\\n                 name: str,\\n                 function: Callable,\\n                 description: str,\\n                 parameters: Dict,\\n                 terminal: bool = False):\\n        self.name = name\\n        self.function = function\\n        self.description = description\\n        self.terminal = terminal\\n        self.parameters = parameters\\n\\n    def execute(self, **args) -> Any:\\n        \"\"\"Execute the action\\'s function\"\"\"\\n        return self.function(**args)\\n\\n\\nclass ActionRegistry:\\n    def __init__(self):\\n        self.actions = {}\\n\\n    def register(self, action: Action):\\n        self.actions[action.name] = action\\n\\n    def get_action(self, name: str) -> Action | None:\\n        return self.actions.get(name, None)\\n\\n    def get_actions(self) -> List[Action]:\\n        \"\"\"Get all registered actions\"\"\"\\n        return list(self.actions.values())\\n\\n\\nclass Memory:\\n    def __init__(self):\\n        self.items = []  # Basic conversation history\\n\\n    def add_memory(self, memory: dict):\\n        \"\"\"Add memory to working memory\"\"\"\\n        self.items.append(memory)\\n\\n    def get_memories(self, limit: int = None) -> List[Dict]:\\n        \"\"\"Get formatted conversation history for prompt\"\"\"\\n        return self.items[:limit]\\n\\n    def copy_without_system_memories(self):\\n        \"\"\"Return a copy of the memory without system memories\"\"\"\\n        filtered_items = [m for m in self.items if m[\"type\"] != \"system\"]\\n        memory = Memory()\\n        memory.items = filtered_items\\n        return memory\\n\\n\\nclass Environment:\\n    def execute_action(self, action: Action, args: dict) -> dict:\\n        \"\"\"Execute an action and return the result.\"\"\"\\n        try:\\n            result = action.execute(**args)\\n            return self.format_result(result)\\n        except Exception as e:\\n            return {\\n                \"tool_executed\": False,\\n                \"error\": str(e),\\n                \"traceback\": traceback.format_exc()\\n            }\\n\\n    def format_result(self, result: Any) -> dict:\\n        \"\"\"Format the result with metadata.\"\"\"\\n        return {\\n            \"tool_executed\": True,\\n            \"result\": result,\\n            \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\\n        }\\n\\n\\nclass AgentLanguage:\\n    def __init__(self):\\n        pass\\n\\n    def construct_prompt(self,\\n                         actions: List[Action],\\n                         environment: Environment,\\n                         goals: List[Goal],\\n                         memory: Memory) -> Prompt:\\n        raise NotImplementedError(\"Subclasses must implement this method\")\\n\\n    def parse_response(self, response: str) -> dict:\\n        raise NotImplementedError(\"Subclasses must implement this method\")\\n\\n\\nclass AgentFunctionCallingActionLanguage(AgentLanguage):\\n\\n    def __init__(self):\\n        super().__init__()\\n\\n    def format_goals(self, goals: List[Goal]) -> List:\\n        # Map all goals to a single string that concatenates their description\\n        # and combine into a single message of type system\\n        sep = \"\\\\n-------------------\\\\n\"\\n        goal_instructions = \"\\\\n\\\\n\".join([f\"{goal.name}:{sep}{goal.description}{sep}\" for goal in goals])\\n        return [\\n            {\"role\": \"system\", \"content\": goal_instructions}\\n        ]\\n\\n    def format_memory(self, memory: Memory) -> List:\\n        \"\"\"Generate response from language model\"\"\"\\n        # Map all environment results to a role:user messages\\n        # Map all assistant messages to a role:assistant messages\\n        # Map all user messages to a role:user messages\\n        items = memory.get_memories()\\n        mapped_items = []\\n        for item in items:\\n\\n            content = item.get(\"content\", None)\\n            if not content:\\n                content = json.dumps(item, indent=4)\\n\\n            if item[\"type\"] == \"assistant\":\\n                mapped_items.append({\"role\": \"assistant\", \"content\": content})\\n            elif item[\"type\"] == \"environment\":\\n                # Map environment results to user messages for Cohere compatibility\\n                mapped_items.append({\"role\": \"user\", \"content\": f\"Tool result: {content}\"})\\n            else:\\n                mapped_items.append({\"role\": \"user\", \"content\": content})\\n\\n        return mapped_items\\n\\n    def format_actions(self, actions: List[Action]) -> List:\\n        \"\"\"Convert actions to LangChain-compatible tool format\"\"\"\\n        \\n        tools = []\\n        for action in actions:\\n            # Convert to OpenAI function format that LangChain can use\\n            tool_def = {\\n                \"type\": \"function\",\\n                \"function\": {\\n                    \"name\": action.name,\\n                    \"description\": action.description[:1024],\\n                    \"parameters\": action.parameters,\\n                }\\n            }\\n            tools.append(tool_def)\\n\\n        return tools\\n\\n    def construct_prompt(self,\\n                         actions: List[Action],\\n                         environment: Environment,\\n                         goals: List[Goal],\\n                         memory: Memory) -> Prompt:\\n\\n        prompt = []\\n        prompt += self.format_goals(goals)\\n        prompt += self.format_memory(memory)\\n\\n        tools = self.format_actions(actions)\\n\\n        return Prompt(messages=prompt, tools=tools)\\n\\n    def adapt_prompt_after_parsing_error(self,\\n                                         prompt: Prompt,\\n                                         response: str,\\n                                         traceback: str,\\n                                         error: Any,\\n                                         retries_left: int) -> Prompt:\\n\\n        return prompt\\n\\n    def parse_response(self, response: str) -> dict:\\n        \"\"\"Parse LLM response into structured format by extracting the ```json block\"\"\"\\n\\n        try:\\n            return json.loads(response)\\n\\n        except Exception as e:\\n            return {\\n                \"tool\": \"terminate\",\\n                \"args\": {\"message\": response}\\n            }\\n\\n\\nclass Agent:\\n    def __init__(self,\\n                 goals: List[Goal],\\n                 agent_language: AgentLanguage,\\n                 action_registry: ActionRegistry,\\n                 generate_response: Callable[[Prompt], str],\\n                 environment: Environment):\\n        \"\"\"\\n        Initialize an agent with its core GAME components\\n        \"\"\"\\n        self.goals = goals\\n        self.generate_response = generate_response\\n        self.agent_language = agent_language\\n        self.actions = action_registry\\n        self.environment = environment\\n\\n    def construct_prompt(self, goals: List[Goal], memory: Memory, actions: ActionRegistry) -> Prompt:\\n        \"\"\"Build prompt with memory context\"\"\"\\n        return self.agent_language.construct_prompt(\\n            actions=actions.get_actions(),\\n            environment=self.environment,\\n            goals=goals,\\n            memory=memory\\n        )\\n\\n    def get_action(self, response):\\n        invocation = self.agent_language.parse_response(response)\\n        action = self.actions.get_action(invocation[\"tool\"])\\n        return action, invocation\\n\\n    def should_terminate(self, response: str) -> bool:\\n        action_def, _ = self.get_action(response)\\n        return action_def.terminal\\n\\n    def set_current_task(self, memory: Memory, task: str):\\n        memory.add_memory({\"type\": \"user\", \"content\": task})\\n\\n    def update_memory(self, memory: Memory, response: str, result: dict):\\n        \"\"\"\\n        Update memory with the agent\\'s decision and the environment\\'s response.\\n        \"\"\"\\n        new_memories = [\\n            {\"type\": \"assistant\", \"content\": response},\\n            {\"type\": \"environment\", \"content\": json.dumps(result)}\\n        ]\\n        for m in new_memories:\\n            memory.add_memory(m)\\n\\n    def prompt_llm_for_action(self, full_prompt: Prompt) -> str:\\n        response = self.generate_response(full_prompt)\\n        return response\\n\\n    def run(self, user_input: str, memory=None, max_iterations: int = 50) -> Memory:\\n        \"\"\"\\n        Execute the GAME loop for this agent with a maximum iteration limit.\\n        \"\"\"\\n        memory = memory or Memory()\\n        self.set_current_task(memory, user_input)\\n\\n        for _ in range(max_iterations):\\n            # Construct a prompt that includes the Goals, Actions, and the current Memory\\n            prompt = self.construct_prompt(self.goals, memory, self.actions)\\n\\n            print(\"Agent thinking...\")\\n            # Generate a response from the agent\\n            response = self.prompt_llm_for_action(prompt)\\n            print(f\"Agent Decision: {response}\")\\n\\n            # Determine which action the agent wants to execute\\n            action, invocation = self.get_action(response)\\n\\n            # Execute the action in the environment\\n            result = self.environment.execute_action(action, invocation[\"args\"])\\n            print(f\"Action Result: {result}\")\\n\\n            # Update the agent\\'s memory with information about what happened\\n            self.update_memory(memory, response, result)\\n\\n            # Check if the agent has decided to terminate\\n            if self.should_terminate(response):\\n                break\\n\\n        return memory\\n\\n\\n# ======================== YOUR CODE STARTS HERE ========================\\n\\n# Define the agent\\'s goals\\ngoals = [\\n    Goal(priority=1, name=\"Gather Information\", description=\"Read each file in the project\"),\\n    Goal(priority=1, name=\"Terminate\", description=\"Call the terminate call when you have read all the files \"\\n         \"and provide the content of the README in the terminate message\")\\n]\\n\\n# Define the agent\\'s language\\nagent_language = AgentFunctionCallingActionLanguage()\\n\\ndef read_project_file(name: str) -> str:\\n    with open(name, \"r\") as f:\\n        return f.read()\\n\\ndef list_project_files() -> List[str]:\\n    try:\\n        # Get all files (not just .py files) to see what\\'s available\\n        all_files = [f for f in os.listdir(\".\") if os.path.isfile(f)]\\n        py_files = [f for f in all_files if f.endswith(\".py\")]\\n        \\n        if not py_files:\\n            # If no .py files, return all text files\\n            text_files = [f for f in all_files if f.endswith((\\'.txt\\', \\'.md\\', \\'.json\\', \\'.yaml\\', \\'.yml\\', \\'.py\\', \\'.js\\', \\'.html\\', \\'.css\\'))]\\n            return sorted(text_files) if text_files else [\"No readable files found in current directory\"]\\n        \\n        return sorted(py_files)\\n    except Exception as e:\\n        return [f\"Error listing files: {str(e)}\"]\\n\\n# Define the action registry and register some actions\\naction_registry = ActionRegistry()\\n\\naction_registry.register(Action(\\n    name=\"list_project_files\",\\n    function=list_project_files,\\n    description=\"Lists all readable files in the project directory (prioritizing Python files, but includes other text files if no Python files exist).\",\\n    parameters={\\n        \"type\": \"object\",\\n        \"properties\": {},\\n        \"required\": []\\n    },\\n    terminal=False\\n))\\n\\naction_registry.register(Action(\\n    name=\"read_project_file\",\\n    function=read_project_file,\\n    description=\"Reads a file from the project.\",\\n    parameters={\\n        \"type\": \"object\",\\n        \"properties\": {\\n            \"name\": {\"type\": \"string\"}\\n        },\\n        \"required\": [\"name\"]\\n    },\\n    terminal=False\\n))\\n\\naction_registry.register(Action(\\n    name=\"terminate\",\\n    function=lambda message: f\"{message}\\\\nTerminating...\",\\n    description=\"Terminates the session and prints the message to the user.\",\\n    parameters={\\n        \"type\": \"object\",\\n        \"properties\": {\\n            \"message\": {\"type\": \"string\"}\\n        },\\n        \"required\": [\"message\"]\\n    },\\n    terminal=True\\n))\\n\\n# Define the environment\\nenvironment = Environment()\\n\\n# Create an agent instance\\nagent = Agent(goals, agent_language, action_registry, generate_response, environment)\\n\\n# Run the agent with user input\\nif __name__ == \"__main__\":\\n    user_input = \"Write a README for this project.\"\\n    final_memory = agent.run(user_input)\\n    \\n    # Print the final memory\\n    print(\"\\\\n\" + \"=\"*60)\\n    print(\"FINAL MEMORY:\")\\n    print(\"=\"*60)\\n    for memory in final_memory.get_memories():\\n        print(memory)\\n        print(\"-\" * 40)\\n', 'timestamp': '2025-07-25T18:06:58+0000'}\n",
            "Agent thinking...\n",
            "DEBUG: Starting generate_response\n",
            "DEBUG: Prompt has 6 messages and 3 tools\n",
            "DEBUG: Cohere model initialized\n",
            "DEBUG: Converted 6 messages\n",
            "DEBUG: Tools present, setting up tool calling\n",
            "DEBUG: Original tools: [{'type': 'function', 'function': {'name': 'read_project_file', 'description': \"Reads and returns the content of a specified project file.\\n\\n    Opens the file in read mode and returns its entire contents as a string.\\n    Raises FileNotFoundError if the file doesn't exist.\\n\\n    Args:\\n        name: The name of the file to read\\n\\n    Returns:\\n        The contents of the file as a string\", 'parameters': {'type': 'object', 'properties': {'name': {'type': 'string'}}, 'required': ['name']}}}, {'type': 'function', 'function': {'name': 'list_project_files', 'description': \"Lists all Python files in the current project directory.\\n\\n    Scans the current directory and returns a sorted list of all files\\n    that end with '.py'.\\n\\n    Returns:\\n        A sorted list of Python filenames\", 'parameters': {'type': 'object', 'properties': {}, 'required': []}}}, {'type': 'function', 'function': {'name': 'terminate', 'description': \"Terminates the agent's execution with a final message.\\n\\n    Args:\\n        message: The final message to return before terminating\\n\\n    Returns:\\n        The message with a termination note appended\", 'parameters': {'type': 'object', 'properties': {'message': {'type': 'string'}}, 'required': ['message']}}}]\n",
            "DEBUG: Formatted tools: [{'title': 'read_project_file', 'description': \"Reads and returns the content of a specified project file.\\n\\n    Opens the file in read mode and returns its entire contents as a string.\\n    Raises FileNotFoundError if the file doesn't exist.\\n\\n    Args:\\n        name: The name of the file to read\\n\\n    Returns:\\n        The contents of the file as a string\", 'properties': {'name': {'type': 'string'}}, 'required': ['name']}, {'title': 'list_project_files', 'description': \"Lists all Python files in the current project directory.\\n\\n    Scans the current directory and returns a sorted list of all files\\n    that end with '.py'.\\n\\n    Returns:\\n        A sorted list of Python filenames\", 'properties': {}, 'required': []}, {'title': 'terminate', 'description': \"Terminates the agent's execution with a final message.\\n\\n    Args:\\n        message: The final message to return before terminating\\n\\n    Returns:\\n        The message with a termination note appended\", 'properties': {'message': {'type': 'string'}}, 'required': ['message']}]\n",
            "DEBUG: About to invoke LLM with tools (this may take 30-60 seconds)\n",
            "DEBUG: Got response from LLM with tools\n",
            "DEBUG: Returning result: # Cohere AI Assistant\n",
            "\n",
            "This project contains the code for an AI assistant that can be used to help users with a variety of tasks. The assistant is designed to be flexible and adaptable, with the ability to learn and grow as it interacts with users.\n",
            "\n",
            "The assistant uses a combination of natural language processing, machine learning, and artificial intelligence techniques to understand and respond to user queries. It can perform a range of tasks, including answering questions, providing information, and performing simple commands.\n",
            "\n",
            "The project is written in Python and uses the Cohere API to interact with the AI model. The code is well-documented and includes a number of functions and classes that can be used to extend the assistant's capabilities.\n",
            "\n",
            "To use the assistant, simply run the main.py file and follow the prompts. The assistant will ask you a series of questions to understand your request and will then provide a response. You can also extend the assistant by adding new functions and classes to the code.\n",
            "Agent Decision: # Cohere AI Assistant\n",
            "\n",
            "This project contains the code for an AI assistant that can be used to help users with a variety of tasks. The assistant is designed to be flexible and adaptable, with the ability to learn and grow as it interacts with users.\n",
            "\n",
            "The assistant uses a combination of natural language processing, machine learning, and artificial intelligence techniques to understand and respond to user queries. It can perform a range of tasks, including answering questions, providing information, and performing simple commands.\n",
            "\n",
            "The project is written in Python and uses the Cohere API to interact with the AI model. The code is well-documented and includes a number of functions and classes that can be used to extend the assistant's capabilities.\n",
            "\n",
            "To use the assistant, simply run the main.py file and follow the prompts. The assistant will ask you a series of questions to understand your request and will then provide a response. You can also extend the assistant by adding new functions and classes to the code.\n",
            "Action Result: {'tool_executed': True, 'result': \"# Cohere AI Assistant\\n\\nThis project contains the code for an AI assistant that can be used to help users with a variety of tasks. The assistant is designed to be flexible and adaptable, with the ability to learn and grow as it interacts with users.\\n\\nThe assistant uses a combination of natural language processing, machine learning, and artificial intelligence techniques to understand and respond to user queries. It can perform a range of tasks, including answering questions, providing information, and performing simple commands.\\n\\nThe project is written in Python and uses the Cohere API to interact with the AI model. The code is well-documented and includes a number of functions and classes that can be used to extend the assistant's capabilities.\\n\\nTo use the assistant, simply run the main.py file and follow the prompts. The assistant will ask you a series of questions to understand your request and will then provide a response. You can also extend the assistant by adding new functions and classes to the code.\\nTerminating...\", 'timestamp': '2025-07-25T18:07:14+0000'}\n",
            "[{'type': 'user', 'content': 'Write a README for this project.'}, {'type': 'assistant', 'content': '{\"tool\": \"list_project_files\", \"args\": {}}'}, {'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": [\"main.py\"], \"timestamp\": \"2025-07-25T18:06:57+0000\"}'}, {'type': 'assistant', 'content': '{\"tool\": \"read_project_file\", \"args\": {\"name\": \"main.py\"}}'}, {'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": \"import os\\\\nfrom dataclasses import dataclass, field\\\\nfrom typing import List, Dict, Any, Callable, Optional\\\\nimport json\\\\nimport time\\\\nimport traceback\\\\nfrom langchain_cohere import ChatCohere\\\\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\\\\n\\\\n@dataclass\\\\nclass Prompt:\\\\n    messages: List[Dict] = field(default_factory=list)\\\\n    tools: List[Dict] = field(default_factory=list)\\\\n    metadata: dict = field(default_factory=dict)  # Fixing mutable default issue\\\\n\\\\n\\\\ndef generate_response(prompt: Prompt) -> str:\\\\n    \\\\\"\\\\\"\\\\\"Call LLM to get response\\\\\"\\\\\"\\\\\"\\\\n    \\\\n    # Initialize Cohere model with LangChain\\\\n    llm = ChatCohere(\\\\n        model=\\\\\"command-r-plus\\\\\",  # or \\\\\"command-r\\\\\" for faster responses\\\\n        max_tokens=1024,\\\\n        temperature=0.3\\\\n    )\\\\n\\\\n    messages = prompt.messages\\\\n    tools = prompt.tools\\\\n\\\\n    result = None\\\\n\\\\n    # Convert dict messages to LangChain message objects\\\\n    langchain_messages = []\\\\n    for msg in messages:\\\\n        if msg[\\\\\"role\\\\\"] == \\\\\"system\\\\\":\\\\n            langchain_messages.append(SystemMessage(content=msg[\\\\\"content\\\\\"]))\\\\n        elif msg[\\\\\"role\\\\\"] == \\\\\"user\\\\\":\\\\n            langchain_messages.append(HumanMessage(content=msg[\\\\\"content\\\\\"]))\\\\n        elif msg[\\\\\"role\\\\\"] == \\\\\"assistant\\\\\":\\\\n            langchain_messages.append(AIMessage(content=msg[\\\\\"content\\\\\"]))\\\\n\\\\n    if not tools:\\\\n        response = llm.invoke(langchain_messages)\\\\n        result = response.content\\\\n    else:\\\\n        # Convert tools to the format expected by LangChain Cohere\\\\n        formatted_tools = []\\\\n        for tool in tools:\\\\n            if isinstance(tool, dict) and \\\\\"function\\\\\" in tool:\\\\n                # Extract the function definition for LangChain\\\\n                func_def = tool[\\\\\"function\\\\\"]\\\\n                # Convert to Cohere-compatible format\\\\n                cohere_tool = {\\\\n                    \\\\\"title\\\\\": func_def[\\\\\"name\\\\\"],\\\\n                    \\\\\"description\\\\\": func_def[\\\\\"description\\\\\"],\\\\n                    \\\\\"properties\\\\\": func_def[\\\\\"parameters\\\\\"].get(\\\\\"properties\\\\\", {}),\\\\n                    \\\\\"required\\\\\": func_def[\\\\\"parameters\\\\\"].get(\\\\\"required\\\\\", [])\\\\n                }\\\\n                formatted_tools.append(cohere_tool)\\\\n        \\\\n        # Bind tools to the model for function calling\\\\n        llm_with_tools = llm.bind_tools(formatted_tools)\\\\n        response = llm_with_tools.invoke(langchain_messages)\\\\n\\\\n        if response.tool_calls:\\\\n            tool_call = response.tool_calls[0]\\\\n            result = {\\\\n                \\\\\"tool\\\\\": tool_call[\\\\\"name\\\\\"],\\\\n                \\\\\"args\\\\\": tool_call[\\\\\"args\\\\\"],\\\\n            }\\\\n            result = json.dumps(result)\\\\n        else:\\\\n            result = response.content\\\\n\\\\n    return result\\\\n\\\\n\\\\n@dataclass(frozen=True)\\\\nclass Goal:\\\\n    priority: int\\\\n    name: str\\\\n    description: str\\\\n\\\\n\\\\nclass Action:\\\\n    def __init__(self,\\\\n                 name: str,\\\\n                 function: Callable,\\\\n                 description: str,\\\\n                 parameters: Dict,\\\\n                 terminal: bool = False):\\\\n        self.name = name\\\\n        self.function = function\\\\n        self.description = description\\\\n        self.terminal = terminal\\\\n        self.parameters = parameters\\\\n\\\\n    def execute(self, **args) -> Any:\\\\n        \\\\\"\\\\\"\\\\\"Execute the action\\'s function\\\\\"\\\\\"\\\\\"\\\\n        return self.function(**args)\\\\n\\\\n\\\\nclass ActionRegistry:\\\\n    def __init__(self):\\\\n        self.actions = {}\\\\n\\\\n    def register(self, action: Action):\\\\n        self.actions[action.name] = action\\\\n\\\\n    def get_action(self, name: str) -> Action | None:\\\\n        return self.actions.get(name, None)\\\\n\\\\n    def get_actions(self) -> List[Action]:\\\\n        \\\\\"\\\\\"\\\\\"Get all registered actions\\\\\"\\\\\"\\\\\"\\\\n        return list(self.actions.values())\\\\n\\\\n\\\\nclass Memory:\\\\n    def __init__(self):\\\\n        self.items = []  # Basic conversation history\\\\n\\\\n    def add_memory(self, memory: dict):\\\\n        \\\\\"\\\\\"\\\\\"Add memory to working memory\\\\\"\\\\\"\\\\\"\\\\n        self.items.append(memory)\\\\n\\\\n    def get_memories(self, limit: int = None) -> List[Dict]:\\\\n        \\\\\"\\\\\"\\\\\"Get formatted conversation history for prompt\\\\\"\\\\\"\\\\\"\\\\n        return self.items[:limit]\\\\n\\\\n    def copy_without_system_memories(self):\\\\n        \\\\\"\\\\\"\\\\\"Return a copy of the memory without system memories\\\\\"\\\\\"\\\\\"\\\\n        filtered_items = [m for m in self.items if m[\\\\\"type\\\\\"] != \\\\\"system\\\\\"]\\\\n        memory = Memory()\\\\n        memory.items = filtered_items\\\\n        return memory\\\\n\\\\n\\\\nclass Environment:\\\\n    def execute_action(self, action: Action, args: dict) -> dict:\\\\n        \\\\\"\\\\\"\\\\\"Execute an action and return the result.\\\\\"\\\\\"\\\\\"\\\\n        try:\\\\n            result = action.execute(**args)\\\\n            return self.format_result(result)\\\\n        except Exception as e:\\\\n            return {\\\\n                \\\\\"tool_executed\\\\\": False,\\\\n                \\\\\"error\\\\\": str(e),\\\\n                \\\\\"traceback\\\\\": traceback.format_exc()\\\\n            }\\\\n\\\\n    def format_result(self, result: Any) -> dict:\\\\n        \\\\\"\\\\\"\\\\\"Format the result with metadata.\\\\\"\\\\\"\\\\\"\\\\n        return {\\\\n            \\\\\"tool_executed\\\\\": True,\\\\n            \\\\\"result\\\\\": result,\\\\n            \\\\\"timestamp\\\\\": time.strftime(\\\\\"%Y-%m-%dT%H:%M:%S%z\\\\\")\\\\n        }\\\\n\\\\n\\\\nclass AgentLanguage:\\\\n    def __init__(self):\\\\n        pass\\\\n\\\\n    def construct_prompt(self,\\\\n                         actions: List[Action],\\\\n                         environment: Environment,\\\\n                         goals: List[Goal],\\\\n                         memory: Memory) -> Prompt:\\\\n        raise NotImplementedError(\\\\\"Subclasses must implement this method\\\\\")\\\\n\\\\n    def parse_response(self, response: str) -> dict:\\\\n        raise NotImplementedError(\\\\\"Subclasses must implement this method\\\\\")\\\\n\\\\n\\\\nclass AgentFunctionCallingActionLanguage(AgentLanguage):\\\\n\\\\n    def __init__(self):\\\\n        super().__init__()\\\\n\\\\n    def format_goals(self, goals: List[Goal]) -> List:\\\\n        # Map all goals to a single string that concatenates their description\\\\n        # and combine into a single message of type system\\\\n        sep = \\\\\"\\\\\\\\n-------------------\\\\\\\\n\\\\\"\\\\n        goal_instructions = \\\\\"\\\\\\\\n\\\\\\\\n\\\\\".join([f\\\\\"{goal.name}:{sep}{goal.description}{sep}\\\\\" for goal in goals])\\\\n        return [\\\\n            {\\\\\"role\\\\\": \\\\\"system\\\\\", \\\\\"content\\\\\": goal_instructions}\\\\n        ]\\\\n\\\\n    def format_memory(self, memory: Memory) -> List:\\\\n        \\\\\"\\\\\"\\\\\"Generate response from language model\\\\\"\\\\\"\\\\\"\\\\n        # Map all environment results to a role:user messages\\\\n        # Map all assistant messages to a role:assistant messages\\\\n        # Map all user messages to a role:user messages\\\\n        items = memory.get_memories()\\\\n        mapped_items = []\\\\n        for item in items:\\\\n\\\\n            content = item.get(\\\\\"content\\\\\", None)\\\\n            if not content:\\\\n                content = json.dumps(item, indent=4)\\\\n\\\\n            if item[\\\\\"type\\\\\"] == \\\\\"assistant\\\\\":\\\\n                mapped_items.append({\\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": content})\\\\n            elif item[\\\\\"type\\\\\"] == \\\\\"environment\\\\\":\\\\n                # Map environment results to user messages for Cohere compatibility\\\\n                mapped_items.append({\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": f\\\\\"Tool result: {content}\\\\\"})\\\\n            else:\\\\n                mapped_items.append({\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": content})\\\\n\\\\n        return mapped_items\\\\n\\\\n    def format_actions(self, actions: List[Action]) -> List:\\\\n        \\\\\"\\\\\"\\\\\"Convert actions to LangChain-compatible tool format\\\\\"\\\\\"\\\\\"\\\\n        \\\\n        tools = []\\\\n        for action in actions:\\\\n            # Convert to OpenAI function format that LangChain can use\\\\n            tool_def = {\\\\n                \\\\\"type\\\\\": \\\\\"function\\\\\",\\\\n                \\\\\"function\\\\\": {\\\\n                    \\\\\"name\\\\\": action.name,\\\\n                    \\\\\"description\\\\\": action.description[:1024],\\\\n                    \\\\\"parameters\\\\\": action.parameters,\\\\n                }\\\\n            }\\\\n            tools.append(tool_def)\\\\n\\\\n        return tools\\\\n\\\\n    def construct_prompt(self,\\\\n                         actions: List[Action],\\\\n                         environment: Environment,\\\\n                         goals: List[Goal],\\\\n                         memory: Memory) -> Prompt:\\\\n\\\\n        prompt = []\\\\n        prompt += self.format_goals(goals)\\\\n        prompt += self.format_memory(memory)\\\\n\\\\n        tools = self.format_actions(actions)\\\\n\\\\n        return Prompt(messages=prompt, tools=tools)\\\\n\\\\n    def adapt_prompt_after_parsing_error(self,\\\\n                                         prompt: Prompt,\\\\n                                         response: str,\\\\n                                         traceback: str,\\\\n                                         error: Any,\\\\n                                         retries_left: int) -> Prompt:\\\\n\\\\n        return prompt\\\\n\\\\n    def parse_response(self, response: str) -> dict:\\\\n        \\\\\"\\\\\"\\\\\"Parse LLM response into structured format by extracting the ```json block\\\\\"\\\\\"\\\\\"\\\\n\\\\n        try:\\\\n            return json.loads(response)\\\\n\\\\n        except Exception as e:\\\\n            return {\\\\n                \\\\\"tool\\\\\": \\\\\"terminate\\\\\",\\\\n                \\\\\"args\\\\\": {\\\\\"message\\\\\": response}\\\\n            }\\\\n\\\\n\\\\nclass Agent:\\\\n    def __init__(self,\\\\n                 goals: List[Goal],\\\\n                 agent_language: AgentLanguage,\\\\n                 action_registry: ActionRegistry,\\\\n                 generate_response: Callable[[Prompt], str],\\\\n                 environment: Environment):\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        Initialize an agent with its core GAME components\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        self.goals = goals\\\\n        self.generate_response = generate_response\\\\n        self.agent_language = agent_language\\\\n        self.actions = action_registry\\\\n        self.environment = environment\\\\n\\\\n    def construct_prompt(self, goals: List[Goal], memory: Memory, actions: ActionRegistry) -> Prompt:\\\\n        \\\\\"\\\\\"\\\\\"Build prompt with memory context\\\\\"\\\\\"\\\\\"\\\\n        return self.agent_language.construct_prompt(\\\\n            actions=actions.get_actions(),\\\\n            environment=self.environment,\\\\n            goals=goals,\\\\n            memory=memory\\\\n        )\\\\n\\\\n    def get_action(self, response):\\\\n        invocation = self.agent_language.parse_response(response)\\\\n        action = self.actions.get_action(invocation[\\\\\"tool\\\\\"])\\\\n        return action, invocation\\\\n\\\\n    def should_terminate(self, response: str) -> bool:\\\\n        action_def, _ = self.get_action(response)\\\\n        return action_def.terminal\\\\n\\\\n    def set_current_task(self, memory: Memory, task: str):\\\\n        memory.add_memory({\\\\\"type\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": task})\\\\n\\\\n    def update_memory(self, memory: Memory, response: str, result: dict):\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        Update memory with the agent\\'s decision and the environment\\'s response.\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        new_memories = [\\\\n            {\\\\\"type\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": response},\\\\n            {\\\\\"type\\\\\": \\\\\"environment\\\\\", \\\\\"content\\\\\": json.dumps(result)}\\\\n        ]\\\\n        for m in new_memories:\\\\n            memory.add_memory(m)\\\\n\\\\n    def prompt_llm_for_action(self, full_prompt: Prompt) -> str:\\\\n        response = self.generate_response(full_prompt)\\\\n        return response\\\\n\\\\n    def run(self, user_input: str, memory=None, max_iterations: int = 50) -> Memory:\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        Execute the GAME loop for this agent with a maximum iteration limit.\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        memory = memory or Memory()\\\\n        self.set_current_task(memory, user_input)\\\\n\\\\n        for _ in range(max_iterations):\\\\n            # Construct a prompt that includes the Goals, Actions, and the current Memory\\\\n            prompt = self.construct_prompt(self.goals, memory, self.actions)\\\\n\\\\n            print(\\\\\"Agent thinking...\\\\\")\\\\n            # Generate a response from the agent\\\\n            response = self.prompt_llm_for_action(prompt)\\\\n            print(f\\\\\"Agent Decision: {response}\\\\\")\\\\n\\\\n            # Determine which action the agent wants to execute\\\\n            action, invocation = self.get_action(response)\\\\n\\\\n            # Execute the action in the environment\\\\n            result = self.environment.execute_action(action, invocation[\\\\\"args\\\\\"])\\\\n            print(f\\\\\"Action Result: {result}\\\\\")\\\\n\\\\n            # Update the agent\\'s memory with information about what happened\\\\n            self.update_memory(memory, response, result)\\\\n\\\\n            # Check if the agent has decided to terminate\\\\n            if self.should_terminate(response):\\\\n                break\\\\n\\\\n        return memory\\\\n\\\\n\\\\n# ======================== YOUR CODE STARTS HERE ========================\\\\n\\\\n# Define the agent\\'s goals\\\\ngoals = [\\\\n    Goal(priority=1, name=\\\\\"Gather Information\\\\\", description=\\\\\"Read each file in the project\\\\\"),\\\\n    Goal(priority=1, name=\\\\\"Terminate\\\\\", description=\\\\\"Call the terminate call when you have read all the files \\\\\"\\\\n         \\\\\"and provide the content of the README in the terminate message\\\\\")\\\\n]\\\\n\\\\n# Define the agent\\'s language\\\\nagent_language = AgentFunctionCallingActionLanguage()\\\\n\\\\ndef read_project_file(name: str) -> str:\\\\n    with open(name, \\\\\"r\\\\\") as f:\\\\n        return f.read()\\\\n\\\\ndef list_project_files() -> List[str]:\\\\n    try:\\\\n        # Get all files (not just .py files) to see what\\'s available\\\\n        all_files = [f for f in os.listdir(\\\\\".\\\\\") if os.path.isfile(f)]\\\\n        py_files = [f for f in all_files if f.endswith(\\\\\".py\\\\\")]\\\\n        \\\\n        if not py_files:\\\\n            # If no .py files, return all text files\\\\n            text_files = [f for f in all_files if f.endswith((\\'.txt\\', \\'.md\\', \\'.json\\', \\'.yaml\\', \\'.yml\\', \\'.py\\', \\'.js\\', \\'.html\\', \\'.css\\'))]\\\\n            return sorted(text_files) if text_files else [\\\\\"No readable files found in current directory\\\\\"]\\\\n        \\\\n        return sorted(py_files)\\\\n    except Exception as e:\\\\n        return [f\\\\\"Error listing files: {str(e)}\\\\\"]\\\\n\\\\n# Define the action registry and register some actions\\\\naction_registry = ActionRegistry()\\\\n\\\\naction_registry.register(Action(\\\\n    name=\\\\\"list_project_files\\\\\",\\\\n    function=list_project_files,\\\\n    description=\\\\\"Lists all readable files in the project directory (prioritizing Python files, but includes other text files if no Python files exist).\\\\\",\\\\n    parameters={\\\\n        \\\\\"type\\\\\": \\\\\"object\\\\\",\\\\n        \\\\\"properties\\\\\": {},\\\\n        \\\\\"required\\\\\": []\\\\n    },\\\\n    terminal=False\\\\n))\\\\n\\\\naction_registry.register(Action(\\\\n    name=\\\\\"read_project_file\\\\\",\\\\n    function=read_project_file,\\\\n    description=\\\\\"Reads a file from the project.\\\\\",\\\\n    parameters={\\\\n        \\\\\"type\\\\\": \\\\\"object\\\\\",\\\\n        \\\\\"properties\\\\\": {\\\\n            \\\\\"name\\\\\": {\\\\\"type\\\\\": \\\\\"string\\\\\"}\\\\n        },\\\\n        \\\\\"required\\\\\": [\\\\\"name\\\\\"]\\\\n    },\\\\n    terminal=False\\\\n))\\\\n\\\\naction_registry.register(Action(\\\\n    name=\\\\\"terminate\\\\\",\\\\n    function=lambda message: f\\\\\"{message}\\\\\\\\nTerminating...\\\\\",\\\\n    description=\\\\\"Terminates the session and prints the message to the user.\\\\\",\\\\n    parameters={\\\\n        \\\\\"type\\\\\": \\\\\"object\\\\\",\\\\n        \\\\\"properties\\\\\": {\\\\n            \\\\\"message\\\\\": {\\\\\"type\\\\\": \\\\\"string\\\\\"}\\\\n        },\\\\n        \\\\\"required\\\\\": [\\\\\"message\\\\\"]\\\\n    },\\\\n    terminal=True\\\\n))\\\\n\\\\n# Define the environment\\\\nenvironment = Environment()\\\\n\\\\n# Create an agent instance\\\\nagent = Agent(goals, agent_language, action_registry, generate_response, environment)\\\\n\\\\n# Run the agent with user input\\\\nif __name__ == \\\\\"__main__\\\\\":\\\\n    user_input = \\\\\"Write a README for this project.\\\\\"\\\\n    final_memory = agent.run(user_input)\\\\n    \\\\n    # Print the final memory\\\\n    print(\\\\\"\\\\\\\\n\\\\\" + \\\\\"=\\\\\"*60)\\\\n    print(\\\\\"FINAL MEMORY:\\\\\")\\\\n    print(\\\\\"=\\\\\"*60)\\\\n    for memory in final_memory.get_memories():\\\\n        print(memory)\\\\n        print(\\\\\"-\\\\\" * 40)\\\\n\", \"timestamp\": \"2025-07-25T18:06:58+0000\"}'}, {'type': 'assistant', 'content': \"# Cohere AI Assistant\\n\\nThis project contains the code for an AI assistant that can be used to help users with a variety of tasks. The assistant is designed to be flexible and adaptable, with the ability to learn and grow as it interacts with users.\\n\\nThe assistant uses a combination of natural language processing, machine learning, and artificial intelligence techniques to understand and respond to user queries. It can perform a range of tasks, including answering questions, providing information, and performing simple commands.\\n\\nThe project is written in Python and uses the Cohere API to interact with the AI model. The code is well-documented and includes a number of functions and classes that can be used to extend the assistant's capabilities.\\n\\nTo use the assistant, simply run the main.py file and follow the prompts. The assistant will ask you a series of questions to understand your request and will then provide a response. You can also extend the assistant by adding new functions and classes to the code.\"}, {'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": \"# Cohere AI Assistant\\\\n\\\\nThis project contains the code for an AI assistant that can be used to help users with a variety of tasks. The assistant is designed to be flexible and adaptable, with the ability to learn and grow as it interacts with users.\\\\n\\\\nThe assistant uses a combination of natural language processing, machine learning, and artificial intelligence techniques to understand and respond to user queries. It can perform a range of tasks, including answering questions, providing information, and performing simple commands.\\\\n\\\\nThe project is written in Python and uses the Cohere API to interact with the AI model. The code is well-documented and includes a number of functions and classes that can be used to extend the assistant\\'s capabilities.\\\\n\\\\nTo use the assistant, simply run the main.py file and follow the prompts. The assistant will ask you a series of questions to understand your request and will then provide a response. You can also extend the assistant by adding new functions and classes to the code.\\\\nTerminating...\", \"timestamp\": \"2025-07-25T18:07:14+0000\"}'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kBZesZu0l-nb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}